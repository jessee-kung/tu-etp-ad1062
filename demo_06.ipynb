{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of Ch6. Generative Adversarial Network\n",
    "----\n",
    "This is the sample code of TU-ETP-AD1062 Machine Learning Fundamentals.\n",
    "\n",
    "For more information, please refer to:\n",
    "https://sites.google.com/view/tu-ad1062-mlfundamentals/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Conv2D, MaxPooling2D, ZeroPadding2D, UpSampling2D, Reshape, Flatten, BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Model\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (60000, 28, 28)\n",
      "Vectorized image shape: (60000, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(I_train, y_train), (I_test, y_test) = mnist.load_data()\n",
    "\n",
    "I_train = np.array(I_train) / 255.0\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "I_test = np.array(I_test) / 255.0\n",
    "y_test = np.array(y_test)\n",
    "print('Image shape: %s' % (I_train.shape, ))\n",
    "\n",
    "X_train = I_train.reshape((len(I_train), np.prod(I_train.shape[1:])))\n",
    "X_test = I_test.reshape((len(I_test), np.prod(I_test.shape[1:])))\n",
    "print('Vectorized image shape: %s' %(X_train.shape, ))\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.suptitle('Original images (First 20)')\n",
    "for i in range(0,20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(I_train[i,:], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Auto-encoder\n",
    "\n",
    "Reference: https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 6.1.1. Auto-Encoder\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 2\n",
    "\n",
    "def build_autoencoder():\n",
    "    input_img = Input(shape=(784,))\n",
    "    \n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "    decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "    # Auto-Encoder\n",
    "    autoencoder = Model(input_img, decoded)\n",
    "\n",
    "    # Encoder\n",
    "    encoder = Model(input_img, encoded)\n",
    "\n",
    "    # Decoder\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    \n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    \n",
    "    return autoencoder, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder, decoder = build_autoencoder()\n",
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=20,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_imgs = encoder.predict(X_train)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.suptitle('Original images')\n",
    "for i in range(0,20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(I_train[i,:], cmap='gray')\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.suptitle('Generated images')\n",
    "for i in range(0,20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(decoded_imgs[i,:].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 6.1.2. Auto-Encoder with Convolutional Layer\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 2\n",
    "\n",
    "def build_cnn_autoencoder():\n",
    "    input_img = Input(shape=(28, 28 ,1))\n",
    "    \n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
    "    \n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x) \n",
    "    x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = Model(input_img, decoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "    display(autoencoder.summary())\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "I_train = I_train.reshape( (len(I_train), 28, 28, 1) )\n",
    "I_test = I_test.reshape( (len(I_test), 28, 28, 1) )\n",
    "\n",
    "autoencoder_cnn = build_cnn_autoencoder()\n",
    "autoencoder_cnn.fit(I_train, I_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=256,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(I_test, I_test))\n",
    "autoencoder_cnn.save('data/demo6/autoencoder_cnn_dim2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_train_predict = autoencoder_cnn.predict(I_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "plt.suptitle('Original images')\n",
    "for i in range(0,20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(I_train[i,:].reshape(28, 28), cmap='gray')\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.suptitle('Generated images')\n",
    "for i in range(0,20):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(I_train_predict[i,:].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 6.1.3. Auto-Encoder with Convolutional Layer\n",
    "---\n",
    "##### `__sampling(z_mean, z_log_var)`\n",
    "Function used for reparameterization Trick, which samples a random varaible $\\epsilon$ (i.e., local variable `epsilon`), then added it into mean value:\n",
    "$$z = \\overline{z} + \\sigma \\odot \\epsilon$$, \n",
    "where $\\epsilon=\\mathcal{N}\\left(0,I \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE:\n",
    "    def __init__(self, original_dim):\n",
    "        self._input_shape = (original_dim, )\n",
    "        self._latent_dim = 2\n",
    "        \n",
    "        self._model = None\n",
    "        \n",
    "        self._encoder = None\n",
    "        self._decoder = None\n",
    "        \n",
    "        self.create_VAE()\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def __sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        \n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "        \n",
    "    def create_VAE(self):\n",
    "        # Encoder\n",
    "        inputs = Input(shape=self._input_shape, name='encoder_input')\n",
    "        \n",
    "        x = Dense(512, activation='relu')(inputs)\n",
    "        \n",
    "        z_mean    = Dense(self._latent_dim, name='z_mean')(x)\n",
    "        z_log_var = Dense(self._latent_dim, name='z_log_var')(x)\n",
    "        z         = Lambda(VAE.__sampling, output_shape=(self._latent_dim,), name='z')([z_mean, z_log_var])\n",
    "        \n",
    "        self._encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "        self._encoder.summary()\n",
    "        \n",
    "        # Decoder\n",
    "        latent_inputs = Input(shape=(self._latent_dim,), name='z_sampling')\n",
    "        x = Dense(512, activation='relu')(latent_inputs)\n",
    "        outputs = Dense(self._input_shape[0], activation='sigmoid')(x)\n",
    "        \n",
    "        self._decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "        self._decoder.summary()\n",
    "        \n",
    "        # Define loss\n",
    "        reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "\n",
    "        reconstruction_loss *= self._input_shape[0]\n",
    "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "        kl_loss = K.sum(kl_loss, axis=-1)\n",
    "        kl_loss *= -0.5\n",
    "        \n",
    "        loss = K.mean(reconstruction_loss + kl_loss)\n",
    "        \n",
    "        # instantiate VAE model\n",
    "        outputs = self._decoder(self._encoder(inputs)[2])\n",
    "        \n",
    "        self._model = Model(inputs, outputs, name='vae_mlp')\n",
    "        self._model.add_loss(loss)\n",
    "        self._model.compile(optimizer='adam')\n",
    "        self._model.summary()\n",
    "        \n",
    "        \n",
    "    def fit(self, X_train, epochs=20, batch_size=64, validation_data=None):\n",
    "        self._model.fit(X_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data)\n",
    "        self._model.save('data/demo6/model_vae.h5')\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        return self._model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 784)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          401920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            1026        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            1026        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z (Lambda)                      (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 403,972\n",
      "Trainable params: 403,972\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               1536      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 784)               402192    \n",
      "=================================================================\n",
      "Total params: 403,728\n",
      "Trainable params: 403,728\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 403972    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 784)               403728    \n",
      "=================================================================\n",
      "Total params: 807,700\n",
      "Trainable params: 807,700\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "You must feed a value for placeholder tensor 'z_sampling_2' with dtype float and shape [?,2]\n\t [[{{node z_sampling_2}} = Placeholder[dtype=DT_FLOAT, shape=[?,2], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c889c78d11db>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_dim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mvae\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-ca3b86241195>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_train, epochs, batch_size, validation_data)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/demo6/model_vae.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'z_sampling_2' with dtype float and shape [?,2]\n\t [[{{node z_sampling_2}} = Placeholder[dtype=DT_FLOAT, shape=[?,2], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "vae = VAE(original_dim=X_train.shape[1])\n",
    "vae.fit(X_train=X_train, epochs=10, batch_size=64, validation_data=(X_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda, Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras.utils import plot_model\n",
    "from keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "\n",
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "def plot_results(models,\n",
    "                 data,\n",
    "                 batch_size=128,\n",
    "                 model_name=\"vae_mnist\"):\n",
    "    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n",
    "    # Arguments\n",
    "        models (tuple): encoder and decoder models\n",
    "        data (tuple): test data and label\n",
    "        batch_size (int): prediction batch size\n",
    "        model_name (string): which model is using this function\n",
    "    \"\"\"\n",
    "\n",
    "    encoder, decoder = models\n",
    "    x_test, y_test = data\n",
    "    os.makedirs(model_name, exist_ok=True)\n",
    "\n",
    "    filename = os.path.join(model_name, \"vae_mean.png\")\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    z_mean, _, _ = encoder.predict(x_test,\n",
    "                                   batch_size=batch_size)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "    filename = os.path.join(model_name, \"digits_over_latent.png\")\n",
    "    # display a 30x30 2D manifold of digits\n",
    "    n = 30\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-4, 4, n)\n",
    "    grid_y = np.linspace(-4, 4, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = decoder.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = (n - 1) * digit_size + start_range + 1\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "image_size = x_train.shape[1]\n",
    "original_dim = image_size * image_size\n",
    "x_train = np.reshape(x_train, [-1, original_dim])\n",
    "x_test = np.reshape(x_test, [-1, original_dim])\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# network parameters\n",
    "input_shape = (original_dim, )\n",
    "intermediate_dim = 512\n",
    "batch_size = 128\n",
    "latent_dim = 2\n",
    "epochs = 50\n",
    "\n",
    "# VAE model = encoder + decoder\n",
    "# build encoder model\n",
    "inputs = Input(shape=input_shape, name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "\n",
    "# use reparameterization trick to push the sampling out as input\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "# instantiate encoder model\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()\n",
    "\n",
    "# build decoder model\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)\n",
    "\n",
    "# instantiate decoder model\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()\n",
    "\n",
    "# instantiate VAE model\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    models = (encoder, decoder)\n",
    "    data = (x_test, y_test)\n",
    "    reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "\n",
    "    reconstruction_loss *= original_dim\n",
    "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "    kl_loss = K.sum(kl_loss, axis=-1)\n",
    "    kl_loss *= -0.5\n",
    "    vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='adam')\n",
    "    vae.summary()\n",
    "    plot_model(vae,\n",
    "               to_file='vae_mlp.png',\n",
    "               show_shapes=True)\n",
    "\n",
    "    \n",
    "    vae.fit(x_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, None))\n",
    "    vae.save_weights('vae_mlp_mnist.h5')\n",
    "\n",
    "    plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_results(models, data, batch_size=batch_size, model_name=\"vae_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        validity = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(256, input_dim=self.latent_dim))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Flatten(input_shape=self.img_shape))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(256))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a batch of new images\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Train the generator (to have the discriminator label samples as valid)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            \n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"data/demo6/GAN/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 533,505\n",
      "Trainable params: 533,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_41 (Dense)             (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 784)               803600    \n",
      "_________________________________________________________________\n",
      "reshape_5 (Reshape)          (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,493,520\n",
      "Trainable params: 1,489,936\n",
      "Non-trainable params: 3,584\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.722510, acc.: 57.81%] [G loss: 0.713640]\n",
      "200 [D loss: 0.301511, acc.: 87.50%] [G loss: 3.493377]\n",
      "400 [D loss: 0.687565, acc.: 40.62%] [G loss: 0.675229]\n",
      "600 [D loss: 0.605415, acc.: 70.31%] [G loss: 0.864383]\n",
      "800 [D loss: 0.640038, acc.: 59.38%] [G loss: 0.809106]\n",
      "1000 [D loss: 0.681970, acc.: 51.56%] [G loss: 0.914998]\n",
      "1200 [D loss: 0.612045, acc.: 57.81%] [G loss: 0.865228]\n",
      "1400 [D loss: 0.599363, acc.: 67.19%] [G loss: 0.918532]\n",
      "1600 [D loss: 0.596997, acc.: 76.56%] [G loss: 0.976717]\n",
      "1800 [D loss: 0.540095, acc.: 78.12%] [G loss: 0.915732]\n",
      "2000 [D loss: 0.539960, acc.: 73.44%] [G loss: 0.975041]\n",
      "2200 [D loss: 0.631861, acc.: 67.19%] [G loss: 0.981319]\n",
      "2400 [D loss: 0.656579, acc.: 62.50%] [G loss: 1.017739]\n",
      "2600 [D loss: 0.538233, acc.: 78.12%] [G loss: 1.128847]\n",
      "2800 [D loss: 0.593919, acc.: 68.75%] [G loss: 1.122087]\n",
      "3000 [D loss: 0.552825, acc.: 75.00%] [G loss: 0.972643]\n",
      "3200 [D loss: 0.591657, acc.: 70.31%] [G loss: 0.915038]\n",
      "3400 [D loss: 0.574609, acc.: 75.00%] [G loss: 1.073888]\n",
      "3600 [D loss: 0.568676, acc.: 68.75%] [G loss: 1.006135]\n",
      "3800 [D loss: 0.544056, acc.: 76.56%] [G loss: 1.098493]\n",
      "4000 [D loss: 0.562942, acc.: 68.75%] [G loss: 0.985676]\n",
      "4200 [D loss: 0.574391, acc.: 68.75%] [G loss: 1.076725]\n",
      "4400 [D loss: 0.573033, acc.: 64.06%] [G loss: 1.020382]\n",
      "4600 [D loss: 0.645622, acc.: 64.06%] [G loss: 0.935739]\n",
      "4800 [D loss: 0.662332, acc.: 68.75%] [G loss: 0.981252]\n",
      "5000 [D loss: 0.603598, acc.: 62.50%] [G loss: 0.985159]\n",
      "5200 [D loss: 0.630405, acc.: 64.06%] [G loss: 1.187861]\n",
      "5400 [D loss: 0.617988, acc.: 64.06%] [G loss: 0.916182]\n",
      "5600 [D loss: 0.590421, acc.: 73.44%] [G loss: 1.011463]\n",
      "5800 [D loss: 0.662888, acc.: 57.81%] [G loss: 1.087043]\n",
      "6000 [D loss: 0.633581, acc.: 65.62%] [G loss: 0.953858]\n",
      "6200 [D loss: 0.670996, acc.: 57.81%] [G loss: 0.943902]\n",
      "6400 [D loss: 0.620775, acc.: 64.06%] [G loss: 1.033602]\n",
      "6600 [D loss: 0.617193, acc.: 65.62%] [G loss: 0.950616]\n",
      "6800 [D loss: 0.714442, acc.: 53.12%] [G loss: 0.973548]\n",
      "7000 [D loss: 0.729267, acc.: 57.81%] [G loss: 1.057598]\n",
      "7200 [D loss: 0.620379, acc.: 68.75%] [G loss: 0.859631]\n",
      "7400 [D loss: 0.613143, acc.: 71.88%] [G loss: 1.021935]\n",
      "7600 [D loss: 0.646301, acc.: 64.06%] [G loss: 0.873548]\n",
      "7800 [D loss: 0.613360, acc.: 68.75%] [G loss: 0.945072]\n",
      "8000 [D loss: 0.640918, acc.: 68.75%] [G loss: 0.961026]\n",
      "8200 [D loss: 0.702380, acc.: 54.69%] [G loss: 0.832336]\n",
      "8400 [D loss: 0.585368, acc.: 70.31%] [G loss: 0.982321]\n",
      "8600 [D loss: 0.653164, acc.: 62.50%] [G loss: 0.996010]\n",
      "8800 [D loss: 0.617695, acc.: 62.50%] [G loss: 1.019362]\n",
      "9000 [D loss: 0.582011, acc.: 75.00%] [G loss: 0.942284]\n",
      "9200 [D loss: 0.604552, acc.: 73.44%] [G loss: 1.006397]\n",
      "9400 [D loss: 0.618556, acc.: 67.19%] [G loss: 1.096556]\n",
      "9600 [D loss: 0.667580, acc.: 54.69%] [G loss: 0.861366]\n",
      "9800 [D loss: 0.559682, acc.: 75.00%] [G loss: 0.988058]\n",
      "10000 [D loss: 0.653088, acc.: 65.62%] [G loss: 1.017654]\n",
      "10200 [D loss: 0.604373, acc.: 67.19%] [G loss: 0.908387]\n",
      "10400 [D loss: 0.646872, acc.: 59.38%] [G loss: 1.038001]\n",
      "10600 [D loss: 0.591336, acc.: 68.75%] [G loss: 0.948977]\n",
      "10800 [D loss: 0.613848, acc.: 65.62%] [G loss: 0.883229]\n",
      "11000 [D loss: 0.640012, acc.: 60.94%] [G loss: 0.919687]\n",
      "11200 [D loss: 0.615123, acc.: 57.81%] [G loss: 0.902734]\n",
      "11400 [D loss: 0.617057, acc.: 67.19%] [G loss: 0.979583]\n",
      "11600 [D loss: 0.613959, acc.: 70.31%] [G loss: 1.045352]\n",
      "11800 [D loss: 0.650954, acc.: 53.12%] [G loss: 1.131086]\n",
      "12000 [D loss: 0.640354, acc.: 64.06%] [G loss: 1.012442]\n",
      "12200 [D loss: 0.607130, acc.: 75.00%] [G loss: 0.970363]\n",
      "12400 [D loss: 0.651453, acc.: 62.50%] [G loss: 0.939541]\n",
      "12600 [D loss: 0.659282, acc.: 54.69%] [G loss: 1.030841]\n",
      "12800 [D loss: 0.589921, acc.: 73.44%] [G loss: 0.927258]\n",
      "13000 [D loss: 0.649820, acc.: 59.38%] [G loss: 1.006700]\n",
      "13200 [D loss: 0.619672, acc.: 68.75%] [G loss: 1.038784]\n",
      "13400 [D loss: 0.659899, acc.: 62.50%] [G loss: 0.935040]\n",
      "13600 [D loss: 0.699426, acc.: 56.25%] [G loss: 0.979199]\n",
      "13800 [D loss: 0.614940, acc.: 65.62%] [G loss: 1.056117]\n",
      "14000 [D loss: 0.616809, acc.: 71.88%] [G loss: 0.984750]\n",
      "14200 [D loss: 0.574684, acc.: 70.31%] [G loss: 0.971415]\n",
      "14400 [D loss: 0.663542, acc.: 62.50%] [G loss: 0.924495]\n",
      "14600 [D loss: 0.686798, acc.: 59.38%] [G loss: 0.926402]\n",
      "14800 [D loss: 0.681006, acc.: 62.50%] [G loss: 0.970220]\n",
      "15000 [D loss: 0.570648, acc.: 70.31%] [G loss: 1.050353]\n",
      "15200 [D loss: 0.635522, acc.: 60.94%] [G loss: 0.953202]\n",
      "15400 [D loss: 0.719482, acc.: 59.38%] [G loss: 0.910375]\n",
      "15600 [D loss: 0.651278, acc.: 62.50%] [G loss: 1.011900]\n",
      "15800 [D loss: 0.721331, acc.: 46.88%] [G loss: 1.094878]\n",
      "16000 [D loss: 0.629480, acc.: 56.25%] [G loss: 0.993349]\n",
      "16200 [D loss: 0.527609, acc.: 75.00%] [G loss: 1.008651]\n",
      "16400 [D loss: 0.592241, acc.: 73.44%] [G loss: 0.965557]\n",
      "16600 [D loss: 0.651402, acc.: 54.69%] [G loss: 0.956136]\n",
      "16800 [D loss: 0.634590, acc.: 60.94%] [G loss: 0.931481]\n",
      "17000 [D loss: 0.607262, acc.: 68.75%] [G loss: 0.986580]\n",
      "17200 [D loss: 0.638113, acc.: 64.06%] [G loss: 0.858297]\n",
      "17400 [D loss: 0.612015, acc.: 67.19%] [G loss: 1.005927]\n",
      "17600 [D loss: 0.619013, acc.: 64.06%] [G loss: 1.038710]\n",
      "17800 [D loss: 0.568580, acc.: 68.75%] [G loss: 0.896500]\n",
      "18000 [D loss: 0.627607, acc.: 64.06%] [G loss: 1.054978]\n",
      "18200 [D loss: 0.658444, acc.: 64.06%] [G loss: 0.963826]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18400 [D loss: 0.738544, acc.: 56.25%] [G loss: 1.110454]\n",
      "18600 [D loss: 0.607316, acc.: 62.50%] [G loss: 0.955649]\n",
      "18800 [D loss: 0.653240, acc.: 65.62%] [G loss: 0.957616]\n",
      "19000 [D loss: 0.620945, acc.: 71.88%] [G loss: 0.990155]\n",
      "19200 [D loss: 0.657948, acc.: 70.31%] [G loss: 0.991224]\n",
      "19400 [D loss: 0.639831, acc.: 60.94%] [G loss: 1.019507]\n",
      "19600 [D loss: 0.651740, acc.: 64.06%] [G loss: 1.007610]\n",
      "19800 [D loss: 0.609737, acc.: 59.38%] [G loss: 0.915603]\n",
      "20000 [D loss: 0.646434, acc.: 53.12%] [G loss: 1.020977]\n",
      "20200 [D loss: 0.635912, acc.: 67.19%] [G loss: 1.099247]\n",
      "20400 [D loss: 0.630495, acc.: 57.81%] [G loss: 1.000307]\n",
      "20600 [D loss: 0.560572, acc.: 64.06%] [G loss: 1.127739]\n",
      "20800 [D loss: 0.720014, acc.: 51.56%] [G loss: 0.954316]\n",
      "21000 [D loss: 0.563333, acc.: 75.00%] [G loss: 0.964737]\n",
      "21200 [D loss: 0.688827, acc.: 60.94%] [G loss: 1.062248]\n",
      "21400 [D loss: 0.554261, acc.: 68.75%] [G loss: 1.036725]\n",
      "21600 [D loss: 0.615255, acc.: 64.06%] [G loss: 0.943368]\n",
      "21800 [D loss: 0.646350, acc.: 64.06%] [G loss: 1.036542]\n",
      "22000 [D loss: 0.577621, acc.: 71.88%] [G loss: 1.008595]\n",
      "22200 [D loss: 0.627001, acc.: 62.50%] [G loss: 0.985025]\n",
      "22400 [D loss: 0.587504, acc.: 70.31%] [G loss: 0.880068]\n",
      "22600 [D loss: 0.701085, acc.: 56.25%] [G loss: 0.980749]\n",
      "22800 [D loss: 0.595053, acc.: 73.44%] [G loss: 1.040476]\n",
      "23000 [D loss: 0.576390, acc.: 70.31%] [G loss: 0.966227]\n",
      "23200 [D loss: 0.687399, acc.: 59.38%] [G loss: 0.953271]\n",
      "23400 [D loss: 0.617629, acc.: 57.81%] [G loss: 1.075771]\n",
      "23600 [D loss: 0.608136, acc.: 57.81%] [G loss: 0.923607]\n",
      "23800 [D loss: 0.671317, acc.: 60.94%] [G loss: 1.176711]\n",
      "24000 [D loss: 0.653805, acc.: 65.62%] [G loss: 1.007799]\n",
      "24200 [D loss: 0.615395, acc.: 64.06%] [G loss: 1.036089]\n",
      "24400 [D loss: 0.622981, acc.: 64.06%] [G loss: 1.090751]\n",
      "24600 [D loss: 0.655452, acc.: 59.38%] [G loss: 1.058031]\n",
      "24800 [D loss: 0.577432, acc.: 70.31%] [G loss: 1.001456]\n",
      "25000 [D loss: 0.735983, acc.: 51.56%] [G loss: 1.116905]\n",
      "25200 [D loss: 0.551831, acc.: 75.00%] [G loss: 0.930354]\n",
      "25400 [D loss: 0.602801, acc.: 70.31%] [G loss: 1.107646]\n",
      "25600 [D loss: 0.698110, acc.: 57.81%] [G loss: 1.097988]\n",
      "25800 [D loss: 0.689690, acc.: 53.12%] [G loss: 1.108799]\n",
      "26000 [D loss: 0.599729, acc.: 70.31%] [G loss: 0.990029]\n",
      "26200 [D loss: 0.688901, acc.: 51.56%] [G loss: 0.918909]\n",
      "26400 [D loss: 0.669313, acc.: 70.31%] [G loss: 0.970237]\n",
      "26600 [D loss: 0.645326, acc.: 62.50%] [G loss: 0.946813]\n",
      "26800 [D loss: 0.650442, acc.: 64.06%] [G loss: 1.027500]\n",
      "27000 [D loss: 0.621305, acc.: 64.06%] [G loss: 1.005032]\n",
      "27200 [D loss: 0.572075, acc.: 68.75%] [G loss: 1.084939]\n",
      "27400 [D loss: 0.702388, acc.: 60.94%] [G loss: 0.990594]\n",
      "27600 [D loss: 0.643031, acc.: 65.62%] [G loss: 0.933376]\n",
      "27800 [D loss: 0.656103, acc.: 53.12%] [G loss: 1.037069]\n",
      "28000 [D loss: 0.576208, acc.: 75.00%] [G loss: 1.172554]\n",
      "28200 [D loss: 0.646390, acc.: 64.06%] [G loss: 1.016121]\n",
      "28400 [D loss: 0.621167, acc.: 64.06%] [G loss: 0.836930]\n",
      "28600 [D loss: 0.604040, acc.: 65.62%] [G loss: 0.973889]\n",
      "28800 [D loss: 0.620785, acc.: 68.75%] [G loss: 0.952193]\n",
      "29000 [D loss: 0.640009, acc.: 65.62%] [G loss: 1.026543]\n",
      "29200 [D loss: 0.604466, acc.: 71.88%] [G loss: 1.094376]\n",
      "29400 [D loss: 0.683002, acc.: 59.38%] [G loss: 0.997781]\n",
      "29600 [D loss: 0.719321, acc.: 62.50%] [G loss: 1.072387]\n",
      "29800 [D loss: 0.693085, acc.: 56.25%] [G loss: 1.006962]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN()\n",
    "gan.train(epochs=30000, batch_size=32, sample_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_26 (LeakyReLU)   (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 7, 7, 32)          4640      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 8, 8, 32)          128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_27 (LeakyReLU)   (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_29 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 100,097\n",
      "Trainable params: 99,649\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_6 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 14, 14, 128)       262272    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 28, 64)        131136    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 28, 1)         1025      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,028,673\n",
      "Trainable params: 1,028,289\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 0.999914] [G loss: 1.000178]\n",
      "1 [D loss: 0.999922] [G loss: 1.000181]\n",
      "2 [D loss: 0.999915] [G loss: 1.000182]\n",
      "3 [D loss: 0.999922] [G loss: 1.000185]\n",
      "4 [D loss: 0.999925] [G loss: 1.000181]\n",
      "5 [D loss: 0.999924] [G loss: 1.000185]\n",
      "6 [D loss: 0.999924] [G loss: 1.000182]\n",
      "7 [D loss: 0.999926] [G loss: 1.000177]\n",
      "8 [D loss: 0.999925] [G loss: 1.000175]\n",
      "9 [D loss: 0.999929] [G loss: 1.000180]\n",
      "10 [D loss: 0.999927] [G loss: 1.000177]\n",
      "11 [D loss: 0.999924] [G loss: 1.000170]\n",
      "12 [D loss: 0.999926] [G loss: 1.000173]\n",
      "13 [D loss: 0.999924] [G loss: 1.000172]\n",
      "14 [D loss: 0.999931] [G loss: 1.000162]\n",
      "15 [D loss: 0.999935] [G loss: 1.000148]\n",
      "16 [D loss: 0.999937] [G loss: 1.000141]\n",
      "17 [D loss: 0.999941] [G loss: 1.000143]\n",
      "18 [D loss: 0.999941] [G loss: 1.000138]\n",
      "19 [D loss: 0.999939] [G loss: 1.000130]\n",
      "20 [D loss: 0.999947] [G loss: 1.000117]\n",
      "21 [D loss: 0.999948] [G loss: 1.000122]\n",
      "22 [D loss: 0.999948] [G loss: 1.000113]\n",
      "23 [D loss: 0.999942] [G loss: 1.000122]\n",
      "24 [D loss: 0.999947] [G loss: 1.000110]\n",
      "25 [D loss: 0.999949] [G loss: 1.000112]\n",
      "26 [D loss: 0.999952] [G loss: 1.000106]\n",
      "27 [D loss: 0.999955] [G loss: 1.000107]\n",
      "28 [D loss: 0.999961] [G loss: 1.000111]\n",
      "29 [D loss: 0.999956] [G loss: 1.000087]\n",
      "30 [D loss: 0.999956] [G loss: 1.000093]\n",
      "31 [D loss: 0.999959] [G loss: 1.000089]\n",
      "32 [D loss: 0.999959] [G loss: 1.000083]\n",
      "33 [D loss: 0.999959] [G loss: 1.000080]\n",
      "34 [D loss: 0.999962] [G loss: 1.000086]\n",
      "35 [D loss: 0.999957] [G loss: 1.000076]\n",
      "36 [D loss: 0.999964] [G loss: 1.000074]\n",
      "37 [D loss: 0.999963] [G loss: 1.000065]\n",
      "38 [D loss: 0.999959] [G loss: 1.000068]\n",
      "39 [D loss: 0.999969] [G loss: 1.000073]\n",
      "40 [D loss: 0.999972] [G loss: 1.000072]\n",
      "41 [D loss: 0.999970] [G loss: 1.000067]\n",
      "42 [D loss: 0.999967] [G loss: 1.000068]\n",
      "43 [D loss: 0.999969] [G loss: 1.000072]\n",
      "44 [D loss: 0.999973] [G loss: 1.000060]\n",
      "45 [D loss: 0.999969] [G loss: 1.000069]\n",
      "46 [D loss: 0.999974] [G loss: 1.000063]\n",
      "47 [D loss: 0.999970] [G loss: 1.000062]\n",
      "48 [D loss: 0.999975] [G loss: 1.000060]\n",
      "49 [D loss: 0.999972] [G loss: 1.000059]\n",
      "50 [D loss: 0.999972] [G loss: 1.000052]\n",
      "51 [D loss: 0.999979] [G loss: 1.000066]\n",
      "52 [D loss: 0.999976] [G loss: 1.000049]\n",
      "53 [D loss: 0.999971] [G loss: 1.000066]\n",
      "54 [D loss: 0.999978] [G loss: 1.000064]\n",
      "55 [D loss: 0.999972] [G loss: 1.000066]\n",
      "56 [D loss: 0.999974] [G loss: 1.000069]\n",
      "57 [D loss: 0.999977] [G loss: 1.000073]\n",
      "58 [D loss: 0.999984] [G loss: 1.000071]\n",
      "59 [D loss: 0.999983] [G loss: 1.000069]\n",
      "60 [D loss: 0.999983] [G loss: 1.000065]\n",
      "61 [D loss: 0.999985] [G loss: 1.000074]\n",
      "62 [D loss: 0.999978] [G loss: 1.000075]\n",
      "63 [D loss: 0.999977] [G loss: 1.000073]\n",
      "64 [D loss: 0.999980] [G loss: 1.000067]\n",
      "65 [D loss: 0.999985] [G loss: 1.000073]\n",
      "66 [D loss: 0.999986] [G loss: 1.000072]\n",
      "67 [D loss: 0.999981] [G loss: 1.000076]\n",
      "68 [D loss: 0.999990] [G loss: 1.000078]\n",
      "69 [D loss: 0.999975] [G loss: 1.000093]\n",
      "70 [D loss: 0.999990] [G loss: 1.000076]\n",
      "71 [D loss: 0.999976] [G loss: 1.000088]\n",
      "72 [D loss: 0.999982] [G loss: 1.000102]\n",
      "73 [D loss: 0.999961] [G loss: 1.000111]\n",
      "74 [D loss: 0.999976] [G loss: 1.000106]\n",
      "75 [D loss: 0.999973] [G loss: 1.000096]\n",
      "76 [D loss: 0.999980] [G loss: 1.000113]\n",
      "77 [D loss: 0.999981] [G loss: 1.000097]\n",
      "78 [D loss: 0.999984] [G loss: 1.000100]\n",
      "79 [D loss: 0.999981] [G loss: 1.000104]\n",
      "80 [D loss: 0.999977] [G loss: 1.000103]\n",
      "81 [D loss: 0.999978] [G loss: 1.000133]\n",
      "82 [D loss: 0.999976] [G loss: 1.000118]\n",
      "83 [D loss: 0.999975] [G loss: 1.000138]\n",
      "84 [D loss: 0.999967] [G loss: 1.000118]\n",
      "85 [D loss: 0.999975] [G loss: 1.000121]\n",
      "86 [D loss: 0.999968] [G loss: 1.000134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 [D loss: 0.999969] [G loss: 1.000132]\n",
      "88 [D loss: 0.999975] [G loss: 1.000126]\n",
      "89 [D loss: 0.999966] [G loss: 1.000125]\n",
      "90 [D loss: 0.999978] [G loss: 1.000119]\n",
      "91 [D loss: 0.999976] [G loss: 1.000121]\n",
      "92 [D loss: 0.999972] [G loss: 1.000116]\n",
      "93 [D loss: 0.999973] [G loss: 1.000130]\n",
      "94 [D loss: 0.999977] [G loss: 1.000130]\n",
      "95 [D loss: 0.999968] [G loss: 1.000137]\n",
      "96 [D loss: 0.999962] [G loss: 1.000117]\n",
      "97 [D loss: 0.999967] [G loss: 1.000118]\n",
      "98 [D loss: 0.999974] [G loss: 1.000118]\n",
      "99 [D loss: 0.999969] [G loss: 1.000117]\n",
      "100 [D loss: 0.999970] [G loss: 1.000118]\n",
      "101 [D loss: 0.999972] [G loss: 1.000119]\n",
      "102 [D loss: 0.999969] [G loss: 1.000132]\n",
      "103 [D loss: 0.999967] [G loss: 1.000110]\n",
      "104 [D loss: 0.999970] [G loss: 1.000110]\n",
      "105 [D loss: 0.999967] [G loss: 1.000117]\n",
      "106 [D loss: 0.999975] [G loss: 1.000114]\n",
      "107 [D loss: 0.999967] [G loss: 1.000095]\n",
      "108 [D loss: 0.999959] [G loss: 1.000105]\n",
      "109 [D loss: 0.999966] [G loss: 1.000107]\n",
      "110 [D loss: 0.999972] [G loss: 1.000111]\n",
      "111 [D loss: 0.999967] [G loss: 1.000102]\n",
      "112 [D loss: 0.999964] [G loss: 1.000096]\n",
      "113 [D loss: 0.999972] [G loss: 1.000099]\n",
      "114 [D loss: 0.999969] [G loss: 1.000098]\n",
      "115 [D loss: 0.999969] [G loss: 1.000103]\n",
      "116 [D loss: 0.999980] [G loss: 1.000104]\n",
      "117 [D loss: 0.999977] [G loss: 1.000096]\n",
      "118 [D loss: 0.999992] [G loss: 1.000072]\n",
      "119 [D loss: 0.999979] [G loss: 1.000084]\n",
      "120 [D loss: 0.999969] [G loss: 1.000099]\n",
      "121 [D loss: 0.999965] [G loss: 1.000083]\n",
      "122 [D loss: 0.999966] [G loss: 1.000094]\n",
      "123 [D loss: 0.999971] [G loss: 1.000087]\n",
      "124 [D loss: 0.999961] [G loss: 1.000099]\n",
      "125 [D loss: 0.999957] [G loss: 1.000092]\n",
      "126 [D loss: 0.999976] [G loss: 1.000082]\n",
      "127 [D loss: 0.999966] [G loss: 1.000095]\n",
      "128 [D loss: 0.999969] [G loss: 1.000086]\n",
      "129 [D loss: 0.999970] [G loss: 1.000093]\n",
      "130 [D loss: 0.999969] [G loss: 1.000083]\n",
      "131 [D loss: 0.999975] [G loss: 1.000090]\n",
      "132 [D loss: 0.999980] [G loss: 1.000075]\n",
      "133 [D loss: 0.999972] [G loss: 1.000097]\n",
      "134 [D loss: 0.999972] [G loss: 1.000085]\n",
      "135 [D loss: 0.999970] [G loss: 1.000086]\n",
      "136 [D loss: 0.999970] [G loss: 1.000088]\n",
      "137 [D loss: 0.999964] [G loss: 1.000089]\n",
      "138 [D loss: 0.999968] [G loss: 1.000091]\n",
      "139 [D loss: 0.999968] [G loss: 1.000074]\n",
      "140 [D loss: 0.999979] [G loss: 1.000072]\n",
      "141 [D loss: 0.999969] [G loss: 1.000069]\n",
      "142 [D loss: 0.999963] [G loss: 1.000092]\n",
      "143 [D loss: 0.999965] [G loss: 1.000087]\n",
      "144 [D loss: 0.999970] [G loss: 1.000079]\n",
      "145 [D loss: 0.999966] [G loss: 1.000054]\n",
      "146 [D loss: 0.999971] [G loss: 1.000070]\n",
      "147 [D loss: 0.999967] [G loss: 1.000081]\n",
      "148 [D loss: 0.999972] [G loss: 1.000063]\n",
      "149 [D loss: 0.999972] [G loss: 1.000071]\n",
      "150 [D loss: 0.999958] [G loss: 1.000066]\n",
      "151 [D loss: 0.999978] [G loss: 1.000076]\n",
      "152 [D loss: 0.999973] [G loss: 1.000081]\n",
      "153 [D loss: 0.999971] [G loss: 1.000071]\n",
      "154 [D loss: 0.999977] [G loss: 1.000085]\n",
      "155 [D loss: 0.999966] [G loss: 1.000076]\n",
      "156 [D loss: 0.999970] [G loss: 1.000073]\n",
      "157 [D loss: 0.999970] [G loss: 1.000076]\n",
      "158 [D loss: 0.999966] [G loss: 1.000068]\n",
      "159 [D loss: 0.999974] [G loss: 1.000068]\n",
      "160 [D loss: 0.999973] [G loss: 1.000065]\n",
      "161 [D loss: 0.999972] [G loss: 1.000080]\n",
      "162 [D loss: 0.999965] [G loss: 1.000086]\n",
      "163 [D loss: 0.999969] [G loss: 1.000076]\n",
      "164 [D loss: 0.999973] [G loss: 1.000076]\n",
      "165 [D loss: 0.999965] [G loss: 1.000087]\n",
      "166 [D loss: 0.999979] [G loss: 1.000087]\n",
      "167 [D loss: 0.999973] [G loss: 1.000067]\n",
      "168 [D loss: 0.999976] [G loss: 1.000056]\n",
      "169 [D loss: 0.999969] [G loss: 1.000066]\n",
      "170 [D loss: 0.999975] [G loss: 1.000079]\n",
      "171 [D loss: 0.999964] [G loss: 1.000079]\n",
      "172 [D loss: 0.999980] [G loss: 1.000074]\n",
      "173 [D loss: 0.999955] [G loss: 1.000080]\n",
      "174 [D loss: 0.999968] [G loss: 1.000072]\n",
      "175 [D loss: 0.999967] [G loss: 1.000068]\n",
      "176 [D loss: 0.999966] [G loss: 1.000075]\n",
      "177 [D loss: 0.999969] [G loss: 1.000080]\n",
      "178 [D loss: 0.999970] [G loss: 1.000074]\n",
      "179 [D loss: 0.999969] [G loss: 1.000070]\n",
      "180 [D loss: 0.999963] [G loss: 1.000076]\n",
      "181 [D loss: 0.999973] [G loss: 1.000064]\n",
      "182 [D loss: 0.999963] [G loss: 1.000070]\n",
      "183 [D loss: 0.999969] [G loss: 1.000068]\n",
      "184 [D loss: 0.999968] [G loss: 1.000072]\n",
      "185 [D loss: 0.999973] [G loss: 1.000073]\n",
      "186 [D loss: 0.999969] [G loss: 1.000065]\n",
      "187 [D loss: 0.999969] [G loss: 1.000077]\n",
      "188 [D loss: 0.999977] [G loss: 1.000075]\n",
      "189 [D loss: 0.999967] [G loss: 1.000076]\n",
      "190 [D loss: 0.999971] [G loss: 1.000061]\n",
      "191 [D loss: 0.999969] [G loss: 1.000062]\n",
      "192 [D loss: 0.999968] [G loss: 1.000053]\n",
      "193 [D loss: 0.999966] [G loss: 1.000061]\n",
      "194 [D loss: 0.999970] [G loss: 1.000067]\n",
      "195 [D loss: 0.999975] [G loss: 1.000071]\n",
      "196 [D loss: 0.999969] [G loss: 1.000070]\n",
      "197 [D loss: 0.999965] [G loss: 1.000070]\n",
      "198 [D loss: 0.999967] [G loss: 1.000070]\n",
      "199 [D loss: 0.999969] [G loss: 1.000066]\n",
      "200 [D loss: 0.999968] [G loss: 1.000066]\n",
      "201 [D loss: 0.999967] [G loss: 1.000072]\n",
      "202 [D loss: 0.999978] [G loss: 1.000075]\n",
      "203 [D loss: 0.999969] [G loss: 1.000059]\n",
      "204 [D loss: 0.999977] [G loss: 1.000077]\n",
      "205 [D loss: 0.999970] [G loss: 1.000081]\n",
      "206 [D loss: 0.999971] [G loss: 1.000070]\n",
      "207 [D loss: 0.999980] [G loss: 1.000076]\n",
      "208 [D loss: 0.999971] [G loss: 1.000069]\n",
      "209 [D loss: 0.999962] [G loss: 1.000074]\n",
      "210 [D loss: 0.999967] [G loss: 1.000066]\n",
      "211 [D loss: 0.999965] [G loss: 1.000066]\n",
      "212 [D loss: 0.999971] [G loss: 1.000072]\n",
      "213 [D loss: 0.999969] [G loss: 1.000072]\n",
      "214 [D loss: 0.999978] [G loss: 1.000072]\n",
      "215 [D loss: 0.999970] [G loss: 1.000080]\n",
      "216 [D loss: 0.999971] [G loss: 1.000060]\n",
      "217 [D loss: 0.999980] [G loss: 1.000061]\n",
      "218 [D loss: 0.999967] [G loss: 1.000073]\n",
      "219 [D loss: 0.999976] [G loss: 1.000059]\n",
      "220 [D loss: 0.999967] [G loss: 1.000070]\n",
      "221 [D loss: 0.999964] [G loss: 1.000065]\n",
      "222 [D loss: 0.999969] [G loss: 1.000075]\n",
      "223 [D loss: 0.999969] [G loss: 1.000072]\n",
      "224 [D loss: 0.999972] [G loss: 1.000067]\n",
      "225 [D loss: 0.999969] [G loss: 1.000062]\n",
      "226 [D loss: 0.999967] [G loss: 1.000067]\n",
      "227 [D loss: 0.999970] [G loss: 1.000068]\n",
      "228 [D loss: 0.999960] [G loss: 1.000066]\n",
      "229 [D loss: 0.999968] [G loss: 1.000057]\n",
      "230 [D loss: 0.999965] [G loss: 1.000066]\n",
      "231 [D loss: 0.999973] [G loss: 1.000068]\n",
      "232 [D loss: 0.999958] [G loss: 1.000069]\n",
      "233 [D loss: 0.999970] [G loss: 1.000066]\n",
      "234 [D loss: 0.999970] [G loss: 1.000075]\n",
      "235 [D loss: 0.999968] [G loss: 1.000060]\n",
      "236 [D loss: 0.999963] [G loss: 1.000071]\n",
      "237 [D loss: 0.999972] [G loss: 1.000074]\n",
      "238 [D loss: 0.999967] [G loss: 1.000069]\n",
      "239 [D loss: 0.999960] [G loss: 1.000083]\n",
      "240 [D loss: 0.999963] [G loss: 1.000071]\n",
      "241 [D loss: 0.999971] [G loss: 1.000066]\n",
      "242 [D loss: 0.999967] [G loss: 1.000074]\n",
      "243 [D loss: 0.999976] [G loss: 1.000066]\n",
      "244 [D loss: 0.999978] [G loss: 1.000062]\n",
      "245 [D loss: 0.999968] [G loss: 1.000066]\n",
      "246 [D loss: 0.999971] [G loss: 1.000076]\n",
      "247 [D loss: 0.999965] [G loss: 1.000071]\n",
      "248 [D loss: 0.999984] [G loss: 1.000091]\n",
      "249 [D loss: 0.999967] [G loss: 1.000069]\n",
      "250 [D loss: 0.999967] [G loss: 1.000075]\n",
      "251 [D loss: 0.999970] [G loss: 1.000072]\n",
      "252 [D loss: 0.999969] [G loss: 1.000061]\n",
      "253 [D loss: 0.999972] [G loss: 1.000055]\n",
      "254 [D loss: 0.999967] [G loss: 1.000060]\n",
      "255 [D loss: 0.999976] [G loss: 1.000057]\n",
      "256 [D loss: 0.999973] [G loss: 1.000076]\n",
      "257 [D loss: 0.999957] [G loss: 1.000065]\n",
      "258 [D loss: 0.999976] [G loss: 1.000093]\n",
      "259 [D loss: 0.999979] [G loss: 1.000054]\n",
      "260 [D loss: 0.999968] [G loss: 1.000075]\n",
      "261 [D loss: 0.999976] [G loss: 1.000065]\n",
      "262 [D loss: 0.999965] [G loss: 1.000069]\n",
      "263 [D loss: 0.999977] [G loss: 1.000068]\n",
      "264 [D loss: 0.999964] [G loss: 1.000066]\n",
      "265 [D loss: 0.999974] [G loss: 1.000079]\n",
      "266 [D loss: 0.999971] [G loss: 1.000071]\n",
      "267 [D loss: 0.999973] [G loss: 1.000066]\n",
      "268 [D loss: 0.999977] [G loss: 1.000080]\n",
      "269 [D loss: 0.999964] [G loss: 1.000059]\n",
      "270 [D loss: 0.999968] [G loss: 1.000080]\n",
      "271 [D loss: 0.999972] [G loss: 1.000076]\n",
      "272 [D loss: 0.999960] [G loss: 1.000057]\n",
      "273 [D loss: 0.999971] [G loss: 1.000049]\n",
      "274 [D loss: 0.999977] [G loss: 1.000057]\n",
      "275 [D loss: 0.999967] [G loss: 1.000075]\n",
      "276 [D loss: 0.999969] [G loss: 1.000080]\n",
      "277 [D loss: 0.999973] [G loss: 1.000067]\n",
      "278 [D loss: 0.999979] [G loss: 1.000078]\n",
      "279 [D loss: 0.999964] [G loss: 1.000059]\n",
      "280 [D loss: 0.999972] [G loss: 1.000074]\n",
      "281 [D loss: 0.999975] [G loss: 1.000059]\n",
      "282 [D loss: 0.999971] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 [D loss: 0.999970] [G loss: 1.000069]\n",
      "284 [D loss: 0.999971] [G loss: 1.000066]\n",
      "285 [D loss: 0.999969] [G loss: 1.000075]\n",
      "286 [D loss: 0.999969] [G loss: 1.000077]\n",
      "287 [D loss: 0.999972] [G loss: 1.000068]\n",
      "288 [D loss: 0.999972] [G loss: 1.000076]\n",
      "289 [D loss: 0.999969] [G loss: 1.000067]\n",
      "290 [D loss: 0.999974] [G loss: 1.000076]\n",
      "291 [D loss: 0.999969] [G loss: 1.000065]\n",
      "292 [D loss: 0.999969] [G loss: 1.000073]\n",
      "293 [D loss: 0.999968] [G loss: 1.000074]\n",
      "294 [D loss: 0.999972] [G loss: 1.000065]\n",
      "295 [D loss: 0.999968] [G loss: 1.000072]\n",
      "296 [D loss: 0.999968] [G loss: 1.000062]\n",
      "297 [D loss: 0.999970] [G loss: 1.000072]\n",
      "298 [D loss: 0.999969] [G loss: 1.000074]\n",
      "299 [D loss: 0.999971] [G loss: 1.000065]\n",
      "300 [D loss: 0.999970] [G loss: 1.000068]\n",
      "301 [D loss: 0.999976] [G loss: 1.000055]\n",
      "302 [D loss: 0.999972] [G loss: 1.000075]\n",
      "303 [D loss: 0.999969] [G loss: 1.000068]\n",
      "304 [D loss: 0.999966] [G loss: 1.000065]\n",
      "305 [D loss: 0.999962] [G loss: 1.000060]\n",
      "306 [D loss: 0.999972] [G loss: 1.000058]\n",
      "307 [D loss: 0.999964] [G loss: 1.000068]\n",
      "308 [D loss: 0.999976] [G loss: 1.000074]\n",
      "309 [D loss: 0.999971] [G loss: 1.000072]\n",
      "310 [D loss: 0.999976] [G loss: 1.000062]\n",
      "311 [D loss: 0.999976] [G loss: 1.000065]\n",
      "312 [D loss: 0.999959] [G loss: 1.000068]\n",
      "313 [D loss: 0.999969] [G loss: 1.000069]\n",
      "314 [D loss: 0.999970] [G loss: 1.000041]\n",
      "315 [D loss: 0.999965] [G loss: 1.000064]\n",
      "316 [D loss: 0.999973] [G loss: 1.000067]\n",
      "317 [D loss: 0.999970] [G loss: 1.000069]\n",
      "318 [D loss: 0.999971] [G loss: 1.000063]\n",
      "319 [D loss: 0.999971] [G loss: 1.000057]\n",
      "320 [D loss: 0.999971] [G loss: 1.000063]\n",
      "321 [D loss: 0.999964] [G loss: 1.000064]\n",
      "322 [D loss: 0.999974] [G loss: 1.000058]\n",
      "323 [D loss: 0.999970] [G loss: 1.000078]\n",
      "324 [D loss: 0.999969] [G loss: 1.000070]\n",
      "325 [D loss: 0.999970] [G loss: 1.000069]\n",
      "326 [D loss: 0.999968] [G loss: 1.000059]\n",
      "327 [D loss: 0.999975] [G loss: 1.000055]\n",
      "328 [D loss: 0.999970] [G loss: 1.000072]\n",
      "329 [D loss: 0.999977] [G loss: 1.000059]\n",
      "330 [D loss: 0.999972] [G loss: 1.000067]\n",
      "331 [D loss: 0.999972] [G loss: 1.000068]\n",
      "332 [D loss: 0.999977] [G loss: 1.000065]\n",
      "333 [D loss: 0.999973] [G loss: 1.000075]\n",
      "334 [D loss: 0.999974] [G loss: 1.000068]\n",
      "335 [D loss: 0.999970] [G loss: 1.000067]\n",
      "336 [D loss: 0.999969] [G loss: 1.000074]\n",
      "337 [D loss: 0.999978] [G loss: 1.000053]\n",
      "338 [D loss: 0.999968] [G loss: 1.000048]\n",
      "339 [D loss: 0.999979] [G loss: 1.000056]\n",
      "340 [D loss: 0.999970] [G loss: 1.000069]\n",
      "341 [D loss: 0.999972] [G loss: 1.000063]\n",
      "342 [D loss: 0.999967] [G loss: 1.000065]\n",
      "343 [D loss: 0.999965] [G loss: 1.000065]\n",
      "344 [D loss: 0.999975] [G loss: 1.000062]\n",
      "345 [D loss: 0.999971] [G loss: 1.000053]\n",
      "346 [D loss: 0.999973] [G loss: 1.000068]\n",
      "347 [D loss: 0.999970] [G loss: 1.000060]\n",
      "348 [D loss: 0.999969] [G loss: 1.000068]\n",
      "349 [D loss: 0.999963] [G loss: 1.000064]\n",
      "350 [D loss: 0.999965] [G loss: 1.000062]\n",
      "351 [D loss: 0.999969] [G loss: 1.000067]\n",
      "352 [D loss: 0.999961] [G loss: 1.000074]\n",
      "353 [D loss: 0.999966] [G loss: 1.000066]\n",
      "354 [D loss: 0.999968] [G loss: 1.000065]\n",
      "355 [D loss: 0.999971] [G loss: 1.000063]\n",
      "356 [D loss: 0.999963] [G loss: 1.000060]\n",
      "357 [D loss: 0.999977] [G loss: 1.000061]\n",
      "358 [D loss: 0.999977] [G loss: 1.000064]\n",
      "359 [D loss: 0.999970] [G loss: 1.000071]\n",
      "360 [D loss: 0.999972] [G loss: 1.000069]\n",
      "361 [D loss: 0.999962] [G loss: 1.000072]\n",
      "362 [D loss: 0.999976] [G loss: 1.000057]\n",
      "363 [D loss: 0.999977] [G loss: 1.000079]\n",
      "364 [D loss: 0.999971] [G loss: 1.000071]\n",
      "365 [D loss: 0.999980] [G loss: 1.000049]\n",
      "366 [D loss: 0.999975] [G loss: 1.000062]\n",
      "367 [D loss: 0.999972] [G loss: 1.000062]\n",
      "368 [D loss: 0.999978] [G loss: 1.000064]\n",
      "369 [D loss: 0.999959] [G loss: 1.000052]\n",
      "370 [D loss: 0.999970] [G loss: 1.000057]\n",
      "371 [D loss: 0.999973] [G loss: 1.000071]\n",
      "372 [D loss: 0.999977] [G loss: 1.000053]\n",
      "373 [D loss: 0.999965] [G loss: 1.000070]\n",
      "374 [D loss: 0.999976] [G loss: 1.000046]\n",
      "375 [D loss: 0.999969] [G loss: 1.000066]\n",
      "376 [D loss: 0.999974] [G loss: 1.000064]\n",
      "377 [D loss: 0.999981] [G loss: 1.000067]\n",
      "378 [D loss: 0.999968] [G loss: 1.000054]\n",
      "379 [D loss: 0.999970] [G loss: 1.000071]\n",
      "380 [D loss: 0.999975] [G loss: 1.000076]\n",
      "381 [D loss: 0.999965] [G loss: 1.000064]\n",
      "382 [D loss: 0.999958] [G loss: 1.000073]\n",
      "383 [D loss: 0.999970] [G loss: 1.000062]\n",
      "384 [D loss: 0.999974] [G loss: 1.000080]\n",
      "385 [D loss: 0.999966] [G loss: 1.000062]\n",
      "386 [D loss: 0.999972] [G loss: 1.000074]\n",
      "387 [D loss: 0.999973] [G loss: 1.000058]\n",
      "388 [D loss: 0.999974] [G loss: 1.000056]\n",
      "389 [D loss: 0.999980] [G loss: 1.000063]\n",
      "390 [D loss: 0.999967] [G loss: 1.000064]\n",
      "391 [D loss: 0.999977] [G loss: 1.000062]\n",
      "392 [D loss: 0.999971] [G loss: 1.000068]\n",
      "393 [D loss: 0.999978] [G loss: 1.000060]\n",
      "394 [D loss: 0.999980] [G loss: 1.000060]\n",
      "395 [D loss: 0.999968] [G loss: 1.000062]\n",
      "396 [D loss: 0.999973] [G loss: 1.000063]\n",
      "397 [D loss: 0.999970] [G loss: 1.000066]\n",
      "398 [D loss: 0.999974] [G loss: 1.000071]\n",
      "399 [D loss: 0.999967] [G loss: 1.000064]\n",
      "400 [D loss: 0.999966] [G loss: 1.000065]\n",
      "401 [D loss: 0.999972] [G loss: 1.000067]\n",
      "402 [D loss: 0.999972] [G loss: 1.000073]\n",
      "403 [D loss: 0.999968] [G loss: 1.000054]\n",
      "404 [D loss: 0.999974] [G loss: 1.000059]\n",
      "405 [D loss: 0.999968] [G loss: 1.000074]\n",
      "406 [D loss: 0.999970] [G loss: 1.000064]\n",
      "407 [D loss: 0.999966] [G loss: 1.000075]\n",
      "408 [D loss: 0.999970] [G loss: 1.000062]\n",
      "409 [D loss: 0.999977] [G loss: 1.000073]\n",
      "410 [D loss: 0.999967] [G loss: 1.000059]\n",
      "411 [D loss: 0.999970] [G loss: 1.000066]\n",
      "412 [D loss: 0.999975] [G loss: 1.000069]\n",
      "413 [D loss: 0.999976] [G loss: 1.000069]\n",
      "414 [D loss: 0.999975] [G loss: 1.000065]\n",
      "415 [D loss: 0.999965] [G loss: 1.000075]\n",
      "416 [D loss: 0.999977] [G loss: 1.000052]\n",
      "417 [D loss: 0.999971] [G loss: 1.000073]\n",
      "418 [D loss: 0.999974] [G loss: 1.000070]\n",
      "419 [D loss: 0.999968] [G loss: 1.000061]\n",
      "420 [D loss: 0.999979] [G loss: 1.000049]\n",
      "421 [D loss: 0.999976] [G loss: 1.000052]\n",
      "422 [D loss: 0.999962] [G loss: 1.000066]\n",
      "423 [D loss: 0.999967] [G loss: 1.000064]\n",
      "424 [D loss: 0.999979] [G loss: 1.000063]\n",
      "425 [D loss: 0.999974] [G loss: 1.000068]\n",
      "426 [D loss: 0.999971] [G loss: 1.000064]\n",
      "427 [D loss: 0.999963] [G loss: 1.000054]\n",
      "428 [D loss: 0.999975] [G loss: 1.000060]\n",
      "429 [D loss: 0.999975] [G loss: 1.000063]\n",
      "430 [D loss: 0.999964] [G loss: 1.000073]\n",
      "431 [D loss: 0.999964] [G loss: 1.000045]\n",
      "432 [D loss: 0.999965] [G loss: 1.000072]\n",
      "433 [D loss: 0.999965] [G loss: 1.000059]\n",
      "434 [D loss: 0.999971] [G loss: 1.000068]\n",
      "435 [D loss: 0.999981] [G loss: 1.000065]\n",
      "436 [D loss: 0.999969] [G loss: 1.000069]\n",
      "437 [D loss: 0.999966] [G loss: 1.000064]\n",
      "438 [D loss: 0.999978] [G loss: 1.000060]\n",
      "439 [D loss: 0.999977] [G loss: 1.000068]\n",
      "440 [D loss: 0.999961] [G loss: 1.000057]\n",
      "441 [D loss: 0.999968] [G loss: 1.000072]\n",
      "442 [D loss: 0.999965] [G loss: 1.000057]\n",
      "443 [D loss: 0.999973] [G loss: 1.000059]\n",
      "444 [D loss: 0.999973] [G loss: 1.000053]\n",
      "445 [D loss: 0.999979] [G loss: 1.000063]\n",
      "446 [D loss: 0.999970] [G loss: 1.000065]\n",
      "447 [D loss: 0.999973] [G loss: 1.000071]\n",
      "448 [D loss: 0.999968] [G loss: 1.000061]\n",
      "449 [D loss: 0.999962] [G loss: 1.000073]\n",
      "450 [D loss: 0.999968] [G loss: 1.000061]\n",
      "451 [D loss: 0.999968] [G loss: 1.000074]\n",
      "452 [D loss: 0.999968] [G loss: 1.000068]\n",
      "453 [D loss: 0.999977] [G loss: 1.000063]\n",
      "454 [D loss: 0.999974] [G loss: 1.000065]\n",
      "455 [D loss: 0.999965] [G loss: 1.000069]\n",
      "456 [D loss: 0.999974] [G loss: 1.000063]\n",
      "457 [D loss: 0.999974] [G loss: 1.000073]\n",
      "458 [D loss: 0.999966] [G loss: 1.000058]\n",
      "459 [D loss: 0.999972] [G loss: 1.000061]\n",
      "460 [D loss: 0.999979] [G loss: 1.000054]\n",
      "461 [D loss: 0.999972] [G loss: 1.000058]\n",
      "462 [D loss: 0.999964] [G loss: 1.000061]\n",
      "463 [D loss: 0.999971] [G loss: 1.000071]\n",
      "464 [D loss: 0.999965] [G loss: 1.000066]\n",
      "465 [D loss: 0.999966] [G loss: 1.000067]\n",
      "466 [D loss: 0.999974] [G loss: 1.000069]\n",
      "467 [D loss: 0.999967] [G loss: 1.000062]\n",
      "468 [D loss: 0.999971] [G loss: 1.000057]\n",
      "469 [D loss: 0.999962] [G loss: 1.000059]\n",
      "470 [D loss: 0.999971] [G loss: 1.000063]\n",
      "471 [D loss: 0.999979] [G loss: 1.000071]\n",
      "472 [D loss: 0.999963] [G loss: 1.000057]\n",
      "473 [D loss: 0.999970] [G loss: 1.000066]\n",
      "474 [D loss: 0.999966] [G loss: 1.000049]\n",
      "475 [D loss: 0.999964] [G loss: 1.000064]\n",
      "476 [D loss: 0.999968] [G loss: 1.000067]\n",
      "477 [D loss: 0.999971] [G loss: 1.000073]\n",
      "478 [D loss: 0.999973] [G loss: 1.000068]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479 [D loss: 0.999970] [G loss: 1.000054]\n",
      "480 [D loss: 0.999975] [G loss: 1.000063]\n",
      "481 [D loss: 0.999963] [G loss: 1.000060]\n",
      "482 [D loss: 0.999969] [G loss: 1.000064]\n",
      "483 [D loss: 0.999968] [G loss: 1.000074]\n",
      "484 [D loss: 0.999964] [G loss: 1.000065]\n",
      "485 [D loss: 0.999970] [G loss: 1.000062]\n",
      "486 [D loss: 0.999968] [G loss: 1.000064]\n",
      "487 [D loss: 0.999973] [G loss: 1.000068]\n",
      "488 [D loss: 0.999974] [G loss: 1.000070]\n",
      "489 [D loss: 0.999966] [G loss: 1.000077]\n",
      "490 [D loss: 0.999970] [G loss: 1.000072]\n",
      "491 [D loss: 0.999970] [G loss: 1.000055]\n",
      "492 [D loss: 0.999977] [G loss: 1.000063]\n",
      "493 [D loss: 0.999969] [G loss: 1.000064]\n",
      "494 [D loss: 0.999979] [G loss: 1.000076]\n",
      "495 [D loss: 0.999974] [G loss: 1.000067]\n",
      "496 [D loss: 0.999969] [G loss: 1.000072]\n",
      "497 [D loss: 0.999966] [G loss: 1.000070]\n",
      "498 [D loss: 0.999967] [G loss: 1.000056]\n",
      "499 [D loss: 0.999967] [G loss: 1.000068]\n",
      "500 [D loss: 0.999966] [G loss: 1.000071]\n",
      "501 [D loss: 0.999972] [G loss: 1.000074]\n",
      "502 [D loss: 0.999971] [G loss: 1.000065]\n",
      "503 [D loss: 0.999970] [G loss: 1.000065]\n",
      "504 [D loss: 0.999969] [G loss: 1.000059]\n",
      "505 [D loss: 0.999970] [G loss: 1.000073]\n",
      "506 [D loss: 0.999963] [G loss: 1.000065]\n",
      "507 [D loss: 0.999975] [G loss: 1.000060]\n",
      "508 [D loss: 0.999967] [G loss: 1.000064]\n",
      "509 [D loss: 0.999968] [G loss: 1.000061]\n",
      "510 [D loss: 0.999974] [G loss: 1.000081]\n",
      "511 [D loss: 0.999970] [G loss: 1.000069]\n",
      "512 [D loss: 0.999981] [G loss: 1.000067]\n",
      "513 [D loss: 0.999968] [G loss: 1.000065]\n",
      "514 [D loss: 0.999977] [G loss: 1.000057]\n",
      "515 [D loss: 0.999970] [G loss: 1.000065]\n",
      "516 [D loss: 0.999968] [G loss: 1.000058]\n",
      "517 [D loss: 0.999979] [G loss: 1.000064]\n",
      "518 [D loss: 0.999974] [G loss: 1.000062]\n",
      "519 [D loss: 0.999964] [G loss: 1.000058]\n",
      "520 [D loss: 0.999976] [G loss: 1.000072]\n",
      "521 [D loss: 0.999966] [G loss: 1.000071]\n",
      "522 [D loss: 0.999974] [G loss: 1.000069]\n",
      "523 [D loss: 0.999979] [G loss: 1.000053]\n",
      "524 [D loss: 0.999975] [G loss: 1.000076]\n",
      "525 [D loss: 0.999963] [G loss: 1.000065]\n",
      "526 [D loss: 0.999965] [G loss: 1.000070]\n",
      "527 [D loss: 0.999966] [G loss: 1.000067]\n",
      "528 [D loss: 0.999971] [G loss: 1.000059]\n",
      "529 [D loss: 0.999971] [G loss: 1.000064]\n",
      "530 [D loss: 0.999974] [G loss: 1.000063]\n",
      "531 [D loss: 0.999967] [G loss: 1.000060]\n",
      "532 [D loss: 0.999972] [G loss: 1.000066]\n",
      "533 [D loss: 0.999967] [G loss: 1.000052]\n",
      "534 [D loss: 0.999972] [G loss: 1.000057]\n",
      "535 [D loss: 0.999978] [G loss: 1.000064]\n",
      "536 [D loss: 0.999964] [G loss: 1.000065]\n",
      "537 [D loss: 0.999970] [G loss: 1.000070]\n",
      "538 [D loss: 0.999964] [G loss: 1.000069]\n",
      "539 [D loss: 0.999965] [G loss: 1.000064]\n",
      "540 [D loss: 0.999968] [G loss: 1.000057]\n",
      "541 [D loss: 0.999976] [G loss: 1.000057]\n",
      "542 [D loss: 0.999955] [G loss: 1.000061]\n",
      "543 [D loss: 0.999969] [G loss: 1.000070]\n",
      "544 [D loss: 0.999972] [G loss: 1.000047]\n",
      "545 [D loss: 0.999967] [G loss: 1.000064]\n",
      "546 [D loss: 0.999959] [G loss: 1.000073]\n",
      "547 [D loss: 0.999962] [G loss: 1.000058]\n",
      "548 [D loss: 0.999970] [G loss: 1.000081]\n",
      "549 [D loss: 0.999965] [G loss: 1.000062]\n",
      "550 [D loss: 0.999976] [G loss: 1.000050]\n",
      "551 [D loss: 0.999975] [G loss: 1.000051]\n",
      "552 [D loss: 0.999971] [G loss: 1.000067]\n",
      "553 [D loss: 0.999971] [G loss: 1.000052]\n",
      "554 [D loss: 0.999982] [G loss: 1.000065]\n",
      "555 [D loss: 0.999984] [G loss: 1.000054]\n",
      "556 [D loss: 0.999962] [G loss: 1.000074]\n",
      "557 [D loss: 0.999968] [G loss: 1.000074]\n",
      "558 [D loss: 0.999979] [G loss: 1.000059]\n",
      "559 [D loss: 0.999974] [G loss: 1.000046]\n",
      "560 [D loss: 0.999969] [G loss: 1.000064]\n",
      "561 [D loss: 0.999962] [G loss: 1.000067]\n",
      "562 [D loss: 0.999963] [G loss: 1.000057]\n",
      "563 [D loss: 0.999975] [G loss: 1.000067]\n",
      "564 [D loss: 0.999965] [G loss: 1.000069]\n",
      "565 [D loss: 0.999964] [G loss: 1.000075]\n",
      "566 [D loss: 0.999967] [G loss: 1.000072]\n",
      "567 [D loss: 0.999965] [G loss: 1.000077]\n",
      "568 [D loss: 0.999966] [G loss: 1.000064]\n",
      "569 [D loss: 0.999965] [G loss: 1.000069]\n",
      "570 [D loss: 0.999969] [G loss: 1.000075]\n",
      "571 [D loss: 0.999967] [G loss: 1.000061]\n",
      "572 [D loss: 0.999972] [G loss: 1.000066]\n",
      "573 [D loss: 0.999975] [G loss: 1.000067]\n",
      "574 [D loss: 0.999958] [G loss: 1.000062]\n",
      "575 [D loss: 0.999969] [G loss: 1.000065]\n",
      "576 [D loss: 0.999974] [G loss: 1.000062]\n",
      "577 [D loss: 0.999972] [G loss: 1.000062]\n",
      "578 [D loss: 0.999974] [G loss: 1.000061]\n",
      "579 [D loss: 0.999971] [G loss: 1.000065]\n",
      "580 [D loss: 0.999972] [G loss: 1.000063]\n",
      "581 [D loss: 0.999971] [G loss: 1.000064]\n",
      "582 [D loss: 0.999976] [G loss: 1.000056]\n",
      "583 [D loss: 0.999971] [G loss: 1.000069]\n",
      "584 [D loss: 0.999972] [G loss: 1.000071]\n",
      "585 [D loss: 0.999971] [G loss: 1.000069]\n",
      "586 [D loss: 0.999972] [G loss: 1.000052]\n",
      "587 [D loss: 0.999976] [G loss: 1.000059]\n",
      "588 [D loss: 0.999967] [G loss: 1.000062]\n",
      "589 [D loss: 0.999972] [G loss: 1.000068]\n",
      "590 [D loss: 0.999969] [G loss: 1.000064]\n",
      "591 [D loss: 0.999976] [G loss: 1.000070]\n",
      "592 [D loss: 0.999966] [G loss: 1.000063]\n",
      "593 [D loss: 0.999971] [G loss: 1.000078]\n",
      "594 [D loss: 0.999965] [G loss: 1.000064]\n",
      "595 [D loss: 0.999970] [G loss: 1.000064]\n",
      "596 [D loss: 0.999967] [G loss: 1.000074]\n",
      "597 [D loss: 0.999971] [G loss: 1.000060]\n",
      "598 [D loss: 0.999969] [G loss: 1.000073]\n",
      "599 [D loss: 0.999971] [G loss: 1.000064]\n",
      "600 [D loss: 0.999983] [G loss: 1.000063]\n",
      "601 [D loss: 0.999970] [G loss: 1.000064]\n",
      "602 [D loss: 0.999971] [G loss: 1.000067]\n",
      "603 [D loss: 0.999965] [G loss: 1.000077]\n",
      "604 [D loss: 0.999969] [G loss: 1.000058]\n",
      "605 [D loss: 0.999973] [G loss: 1.000066]\n",
      "606 [D loss: 0.999975] [G loss: 1.000072]\n",
      "607 [D loss: 0.999972] [G loss: 1.000065]\n",
      "608 [D loss: 0.999976] [G loss: 1.000076]\n",
      "609 [D loss: 0.999969] [G loss: 1.000080]\n",
      "610 [D loss: 0.999972] [G loss: 1.000064]\n",
      "611 [D loss: 0.999970] [G loss: 1.000068]\n",
      "612 [D loss: 0.999974] [G loss: 1.000063]\n",
      "613 [D loss: 0.999971] [G loss: 1.000049]\n",
      "614 [D loss: 0.999967] [G loss: 1.000064]\n",
      "615 [D loss: 0.999972] [G loss: 1.000067]\n",
      "616 [D loss: 0.999968] [G loss: 1.000064]\n",
      "617 [D loss: 0.999973] [G loss: 1.000071]\n",
      "618 [D loss: 0.999971] [G loss: 1.000063]\n",
      "619 [D loss: 0.999962] [G loss: 1.000072]\n",
      "620 [D loss: 0.999968] [G loss: 1.000056]\n",
      "621 [D loss: 0.999977] [G loss: 1.000068]\n",
      "622 [D loss: 0.999968] [G loss: 1.000067]\n",
      "623 [D loss: 0.999971] [G loss: 1.000068]\n",
      "624 [D loss: 0.999971] [G loss: 1.000059]\n",
      "625 [D loss: 0.999971] [G loss: 1.000057]\n",
      "626 [D loss: 0.999974] [G loss: 1.000077]\n",
      "627 [D loss: 0.999971] [G loss: 1.000066]\n",
      "628 [D loss: 0.999969] [G loss: 1.000076]\n",
      "629 [D loss: 0.999964] [G loss: 1.000058]\n",
      "630 [D loss: 0.999963] [G loss: 1.000069]\n",
      "631 [D loss: 0.999974] [G loss: 1.000066]\n",
      "632 [D loss: 0.999971] [G loss: 1.000071]\n",
      "633 [D loss: 0.999966] [G loss: 1.000068]\n",
      "634 [D loss: 0.999973] [G loss: 1.000054]\n",
      "635 [D loss: 0.999973] [G loss: 1.000060]\n",
      "636 [D loss: 0.999969] [G loss: 1.000062]\n",
      "637 [D loss: 0.999979] [G loss: 1.000066]\n",
      "638 [D loss: 0.999964] [G loss: 1.000077]\n",
      "639 [D loss: 0.999969] [G loss: 1.000074]\n",
      "640 [D loss: 0.999968] [G loss: 1.000061]\n",
      "641 [D loss: 0.999964] [G loss: 1.000069]\n",
      "642 [D loss: 0.999967] [G loss: 1.000061]\n",
      "643 [D loss: 0.999974] [G loss: 1.000062]\n",
      "644 [D loss: 0.999968] [G loss: 1.000063]\n",
      "645 [D loss: 0.999963] [G loss: 1.000063]\n",
      "646 [D loss: 0.999965] [G loss: 1.000061]\n",
      "647 [D loss: 0.999977] [G loss: 1.000060]\n",
      "648 [D loss: 0.999969] [G loss: 1.000059]\n",
      "649 [D loss: 0.999965] [G loss: 1.000053]\n",
      "650 [D loss: 0.999966] [G loss: 1.000053]\n",
      "651 [D loss: 0.999973] [G loss: 1.000060]\n",
      "652 [D loss: 0.999973] [G loss: 1.000064]\n",
      "653 [D loss: 0.999962] [G loss: 1.000048]\n",
      "654 [D loss: 0.999972] [G loss: 1.000075]\n",
      "655 [D loss: 0.999967] [G loss: 1.000053]\n",
      "656 [D loss: 0.999958] [G loss: 1.000062]\n",
      "657 [D loss: 0.999966] [G loss: 1.000065]\n",
      "658 [D loss: 0.999964] [G loss: 1.000057]\n",
      "659 [D loss: 0.999969] [G loss: 1.000058]\n",
      "660 [D loss: 0.999976] [G loss: 1.000068]\n",
      "661 [D loss: 0.999970] [G loss: 1.000069]\n",
      "662 [D loss: 0.999976] [G loss: 1.000078]\n",
      "663 [D loss: 0.999966] [G loss: 1.000069]\n",
      "664 [D loss: 0.999969] [G loss: 1.000051]\n",
      "665 [D loss: 0.999969] [G loss: 1.000066]\n",
      "666 [D loss: 0.999961] [G loss: 1.000066]\n",
      "667 [D loss: 0.999967] [G loss: 1.000057]\n",
      "668 [D loss: 0.999973] [G loss: 1.000074]\n",
      "669 [D loss: 0.999971] [G loss: 1.000063]\n",
      "670 [D loss: 0.999978] [G loss: 1.000082]\n",
      "671 [D loss: 0.999974] [G loss: 1.000071]\n",
      "672 [D loss: 0.999976] [G loss: 1.000064]\n",
      "673 [D loss: 0.999968] [G loss: 1.000066]\n",
      "674 [D loss: 0.999974] [G loss: 1.000049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675 [D loss: 0.999968] [G loss: 1.000064]\n",
      "676 [D loss: 0.999962] [G loss: 1.000068]\n",
      "677 [D loss: 0.999976] [G loss: 1.000054]\n",
      "678 [D loss: 0.999967] [G loss: 1.000057]\n",
      "679 [D loss: 0.999967] [G loss: 1.000062]\n",
      "680 [D loss: 0.999976] [G loss: 1.000064]\n",
      "681 [D loss: 0.999969] [G loss: 1.000060]\n",
      "682 [D loss: 0.999970] [G loss: 1.000070]\n",
      "683 [D loss: 0.999974] [G loss: 1.000076]\n",
      "684 [D loss: 0.999974] [G loss: 1.000074]\n",
      "685 [D loss: 0.999971] [G loss: 1.000066]\n",
      "686 [D loss: 0.999970] [G loss: 1.000072]\n",
      "687 [D loss: 0.999970] [G loss: 1.000060]\n",
      "688 [D loss: 0.999970] [G loss: 1.000067]\n",
      "689 [D loss: 0.999966] [G loss: 1.000070]\n",
      "690 [D loss: 0.999978] [G loss: 1.000071]\n",
      "691 [D loss: 0.999962] [G loss: 1.000073]\n",
      "692 [D loss: 0.999974] [G loss: 1.000059]\n",
      "693 [D loss: 0.999968] [G loss: 1.000069]\n",
      "694 [D loss: 0.999973] [G loss: 1.000061]\n",
      "695 [D loss: 0.999971] [G loss: 1.000066]\n",
      "696 [D loss: 0.999973] [G loss: 1.000056]\n",
      "697 [D loss: 0.999968] [G loss: 1.000071]\n",
      "698 [D loss: 0.999968] [G loss: 1.000060]\n",
      "699 [D loss: 0.999968] [G loss: 1.000062]\n",
      "700 [D loss: 0.999970] [G loss: 1.000064]\n",
      "701 [D loss: 0.999975] [G loss: 1.000063]\n",
      "702 [D loss: 0.999973] [G loss: 1.000062]\n",
      "703 [D loss: 0.999972] [G loss: 1.000067]\n",
      "704 [D loss: 0.999967] [G loss: 1.000072]\n",
      "705 [D loss: 0.999960] [G loss: 1.000058]\n",
      "706 [D loss: 0.999975] [G loss: 1.000068]\n",
      "707 [D loss: 0.999972] [G loss: 1.000065]\n",
      "708 [D loss: 0.999972] [G loss: 1.000066]\n",
      "709 [D loss: 0.999981] [G loss: 1.000066]\n",
      "710 [D loss: 0.999966] [G loss: 1.000057]\n",
      "711 [D loss: 0.999972] [G loss: 1.000068]\n",
      "712 [D loss: 0.999979] [G loss: 1.000069]\n",
      "713 [D loss: 0.999969] [G loss: 1.000061]\n",
      "714 [D loss: 0.999967] [G loss: 1.000071]\n",
      "715 [D loss: 0.999965] [G loss: 1.000062]\n",
      "716 [D loss: 0.999973] [G loss: 1.000066]\n",
      "717 [D loss: 0.999971] [G loss: 1.000061]\n",
      "718 [D loss: 0.999974] [G loss: 1.000064]\n",
      "719 [D loss: 0.999969] [G loss: 1.000070]\n",
      "720 [D loss: 0.999975] [G loss: 1.000060]\n",
      "721 [D loss: 0.999969] [G loss: 1.000065]\n",
      "722 [D loss: 0.999968] [G loss: 1.000065]\n",
      "723 [D loss: 0.999967] [G loss: 1.000069]\n",
      "724 [D loss: 0.999968] [G loss: 1.000063]\n",
      "725 [D loss: 0.999969] [G loss: 1.000068]\n",
      "726 [D loss: 0.999969] [G loss: 1.000063]\n",
      "727 [D loss: 0.999964] [G loss: 1.000060]\n",
      "728 [D loss: 0.999978] [G loss: 1.000066]\n",
      "729 [D loss: 0.999974] [G loss: 1.000064]\n",
      "730 [D loss: 0.999975] [G loss: 1.000076]\n",
      "731 [D loss: 0.999974] [G loss: 1.000077]\n",
      "732 [D loss: 0.999979] [G loss: 1.000052]\n",
      "733 [D loss: 0.999974] [G loss: 1.000059]\n",
      "734 [D loss: 0.999971] [G loss: 1.000067]\n",
      "735 [D loss: 0.999969] [G loss: 1.000066]\n",
      "736 [D loss: 0.999966] [G loss: 1.000055]\n",
      "737 [D loss: 0.999970] [G loss: 1.000063]\n",
      "738 [D loss: 0.999964] [G loss: 1.000071]\n",
      "739 [D loss: 0.999970] [G loss: 1.000059]\n",
      "740 [D loss: 0.999967] [G loss: 1.000070]\n",
      "741 [D loss: 0.999969] [G loss: 1.000068]\n",
      "742 [D loss: 0.999969] [G loss: 1.000067]\n",
      "743 [D loss: 0.999977] [G loss: 1.000066]\n",
      "744 [D loss: 0.999975] [G loss: 1.000067]\n",
      "745 [D loss: 0.999976] [G loss: 1.000060]\n",
      "746 [D loss: 0.999972] [G loss: 1.000063]\n",
      "747 [D loss: 0.999968] [G loss: 1.000061]\n",
      "748 [D loss: 0.999972] [G loss: 1.000065]\n",
      "749 [D loss: 0.999968] [G loss: 1.000074]\n",
      "750 [D loss: 0.999966] [G loss: 1.000072]\n",
      "751 [D loss: 0.999973] [G loss: 1.000061]\n",
      "752 [D loss: 0.999968] [G loss: 1.000063]\n",
      "753 [D loss: 0.999959] [G loss: 1.000070]\n",
      "754 [D loss: 0.999967] [G loss: 1.000071]\n",
      "755 [D loss: 0.999969] [G loss: 1.000077]\n",
      "756 [D loss: 0.999965] [G loss: 1.000066]\n",
      "757 [D loss: 0.999976] [G loss: 1.000061]\n",
      "758 [D loss: 0.999968] [G loss: 1.000067]\n",
      "759 [D loss: 0.999971] [G loss: 1.000067]\n",
      "760 [D loss: 0.999971] [G loss: 1.000063]\n",
      "761 [D loss: 0.999972] [G loss: 1.000068]\n",
      "762 [D loss: 0.999971] [G loss: 1.000060]\n",
      "763 [D loss: 0.999978] [G loss: 1.000059]\n",
      "764 [D loss: 0.999973] [G loss: 1.000059]\n",
      "765 [D loss: 0.999967] [G loss: 1.000059]\n",
      "766 [D loss: 0.999963] [G loss: 1.000073]\n",
      "767 [D loss: 0.999971] [G loss: 1.000063]\n",
      "768 [D loss: 0.999976] [G loss: 1.000057]\n",
      "769 [D loss: 0.999969] [G loss: 1.000071]\n",
      "770 [D loss: 0.999973] [G loss: 1.000072]\n",
      "771 [D loss: 0.999963] [G loss: 1.000055]\n",
      "772 [D loss: 0.999973] [G loss: 1.000055]\n",
      "773 [D loss: 0.999971] [G loss: 1.000076]\n",
      "774 [D loss: 0.999970] [G loss: 1.000067]\n",
      "775 [D loss: 0.999969] [G loss: 1.000070]\n",
      "776 [D loss: 0.999969] [G loss: 1.000058]\n",
      "777 [D loss: 0.999969] [G loss: 1.000063]\n",
      "778 [D loss: 0.999968] [G loss: 1.000067]\n",
      "779 [D loss: 0.999973] [G loss: 1.000070]\n",
      "780 [D loss: 0.999969] [G loss: 1.000065]\n",
      "781 [D loss: 0.999981] [G loss: 1.000060]\n",
      "782 [D loss: 0.999959] [G loss: 1.000062]\n",
      "783 [D loss: 0.999969] [G loss: 1.000057]\n",
      "784 [D loss: 0.999967] [G loss: 1.000065]\n",
      "785 [D loss: 0.999970] [G loss: 1.000063]\n",
      "786 [D loss: 0.999973] [G loss: 1.000061]\n",
      "787 [D loss: 0.999966] [G loss: 1.000059]\n",
      "788 [D loss: 0.999974] [G loss: 1.000054]\n",
      "789 [D loss: 0.999967] [G loss: 1.000057]\n",
      "790 [D loss: 0.999973] [G loss: 1.000068]\n",
      "791 [D loss: 0.999971] [G loss: 1.000072]\n",
      "792 [D loss: 0.999973] [G loss: 1.000072]\n",
      "793 [D loss: 0.999968] [G loss: 1.000066]\n",
      "794 [D loss: 0.999974] [G loss: 1.000071]\n",
      "795 [D loss: 0.999974] [G loss: 1.000071]\n",
      "796 [D loss: 0.999976] [G loss: 1.000058]\n",
      "797 [D loss: 0.999973] [G loss: 1.000059]\n",
      "798 [D loss: 0.999969] [G loss: 1.000069]\n",
      "799 [D loss: 0.999969] [G loss: 1.000056]\n",
      "800 [D loss: 0.999976] [G loss: 1.000065]\n",
      "801 [D loss: 0.999966] [G loss: 1.000062]\n",
      "802 [D loss: 0.999969] [G loss: 1.000070]\n",
      "803 [D loss: 0.999970] [G loss: 1.000063]\n",
      "804 [D loss: 0.999976] [G loss: 1.000059]\n",
      "805 [D loss: 0.999969] [G loss: 1.000065]\n",
      "806 [D loss: 0.999967] [G loss: 1.000076]\n",
      "807 [D loss: 0.999969] [G loss: 1.000066]\n",
      "808 [D loss: 0.999966] [G loss: 1.000064]\n",
      "809 [D loss: 0.999972] [G loss: 1.000057]\n",
      "810 [D loss: 0.999973] [G loss: 1.000062]\n",
      "811 [D loss: 0.999966] [G loss: 1.000054]\n",
      "812 [D loss: 0.999969] [G loss: 1.000060]\n",
      "813 [D loss: 0.999973] [G loss: 1.000069]\n",
      "814 [D loss: 0.999962] [G loss: 1.000061]\n",
      "815 [D loss: 0.999969] [G loss: 1.000065]\n",
      "816 [D loss: 0.999966] [G loss: 1.000062]\n",
      "817 [D loss: 0.999970] [G loss: 1.000064]\n",
      "818 [D loss: 0.999965] [G loss: 1.000067]\n",
      "819 [D loss: 0.999970] [G loss: 1.000070]\n",
      "820 [D loss: 0.999975] [G loss: 1.000068]\n",
      "821 [D loss: 0.999965] [G loss: 1.000052]\n",
      "822 [D loss: 0.999959] [G loss: 1.000065]\n",
      "823 [D loss: 0.999968] [G loss: 1.000058]\n",
      "824 [D loss: 0.999972] [G loss: 1.000073]\n",
      "825 [D loss: 0.999972] [G loss: 1.000064]\n",
      "826 [D loss: 0.999965] [G loss: 1.000065]\n",
      "827 [D loss: 0.999975] [G loss: 1.000058]\n",
      "828 [D loss: 0.999955] [G loss: 1.000064]\n",
      "829 [D loss: 0.999967] [G loss: 1.000068]\n",
      "830 [D loss: 0.999969] [G loss: 1.000065]\n",
      "831 [D loss: 0.999969] [G loss: 1.000060]\n",
      "832 [D loss: 0.999971] [G loss: 1.000075]\n",
      "833 [D loss: 0.999974] [G loss: 1.000067]\n",
      "834 [D loss: 0.999977] [G loss: 1.000057]\n",
      "835 [D loss: 0.999970] [G loss: 1.000071]\n",
      "836 [D loss: 0.999970] [G loss: 1.000064]\n",
      "837 [D loss: 0.999966] [G loss: 1.000071]\n",
      "838 [D loss: 0.999972] [G loss: 1.000061]\n",
      "839 [D loss: 0.999971] [G loss: 1.000070]\n",
      "840 [D loss: 0.999966] [G loss: 1.000057]\n",
      "841 [D loss: 0.999963] [G loss: 1.000063]\n",
      "842 [D loss: 0.999970] [G loss: 1.000066]\n",
      "843 [D loss: 0.999962] [G loss: 1.000061]\n",
      "844 [D loss: 0.999977] [G loss: 1.000063]\n",
      "845 [D loss: 0.999971] [G loss: 1.000067]\n",
      "846 [D loss: 0.999974] [G loss: 1.000064]\n",
      "847 [D loss: 0.999978] [G loss: 1.000057]\n",
      "848 [D loss: 0.999960] [G loss: 1.000059]\n",
      "849 [D loss: 0.999964] [G loss: 1.000067]\n",
      "850 [D loss: 0.999979] [G loss: 1.000057]\n",
      "851 [D loss: 0.999962] [G loss: 1.000062]\n",
      "852 [D loss: 0.999972] [G loss: 1.000063]\n",
      "853 [D loss: 0.999971] [G loss: 1.000063]\n",
      "854 [D loss: 0.999967] [G loss: 1.000073]\n",
      "855 [D loss: 0.999962] [G loss: 1.000062]\n",
      "856 [D loss: 0.999971] [G loss: 1.000068]\n",
      "857 [D loss: 0.999982] [G loss: 1.000064]\n",
      "858 [D loss: 0.999972] [G loss: 1.000070]\n",
      "859 [D loss: 0.999964] [G loss: 1.000057]\n",
      "860 [D loss: 0.999967] [G loss: 1.000059]\n",
      "861 [D loss: 0.999974] [G loss: 1.000068]\n",
      "862 [D loss: 0.999964] [G loss: 1.000058]\n",
      "863 [D loss: 0.999970] [G loss: 1.000065]\n",
      "864 [D loss: 0.999974] [G loss: 1.000059]\n",
      "865 [D loss: 0.999969] [G loss: 1.000053]\n",
      "866 [D loss: 0.999960] [G loss: 1.000075]\n",
      "867 [D loss: 0.999968] [G loss: 1.000062]\n",
      "868 [D loss: 0.999974] [G loss: 1.000062]\n",
      "869 [D loss: 0.999979] [G loss: 1.000065]\n",
      "870 [D loss: 0.999975] [G loss: 1.000064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "871 [D loss: 0.999970] [G loss: 1.000059]\n",
      "872 [D loss: 0.999975] [G loss: 1.000053]\n",
      "873 [D loss: 0.999967] [G loss: 1.000065]\n",
      "874 [D loss: 0.999968] [G loss: 1.000058]\n",
      "875 [D loss: 0.999968] [G loss: 1.000062]\n",
      "876 [D loss: 0.999978] [G loss: 1.000061]\n",
      "877 [D loss: 0.999966] [G loss: 1.000072]\n",
      "878 [D loss: 0.999960] [G loss: 1.000068]\n",
      "879 [D loss: 0.999975] [G loss: 1.000072]\n",
      "880 [D loss: 0.999960] [G loss: 1.000056]\n",
      "881 [D loss: 0.999971] [G loss: 1.000079]\n",
      "882 [D loss: 0.999965] [G loss: 1.000054]\n",
      "883 [D loss: 0.999974] [G loss: 1.000066]\n",
      "884 [D loss: 0.999964] [G loss: 1.000065]\n",
      "885 [D loss: 0.999967] [G loss: 1.000061]\n",
      "886 [D loss: 0.999965] [G loss: 1.000069]\n",
      "887 [D loss: 0.999970] [G loss: 1.000061]\n",
      "888 [D loss: 0.999964] [G loss: 1.000066]\n",
      "889 [D loss: 0.999970] [G loss: 1.000066]\n",
      "890 [D loss: 0.999982] [G loss: 1.000064]\n",
      "891 [D loss: 0.999972] [G loss: 1.000068]\n",
      "892 [D loss: 0.999972] [G loss: 1.000057]\n",
      "893 [D loss: 0.999973] [G loss: 1.000061]\n",
      "894 [D loss: 0.999969] [G loss: 1.000048]\n",
      "895 [D loss: 0.999969] [G loss: 1.000076]\n",
      "896 [D loss: 0.999970] [G loss: 1.000063]\n",
      "897 [D loss: 0.999968] [G loss: 1.000070]\n",
      "898 [D loss: 0.999978] [G loss: 1.000070]\n",
      "899 [D loss: 0.999975] [G loss: 1.000067]\n",
      "900 [D loss: 0.999973] [G loss: 1.000056]\n",
      "901 [D loss: 0.999968] [G loss: 1.000056]\n",
      "902 [D loss: 0.999960] [G loss: 1.000074]\n",
      "903 [D loss: 0.999973] [G loss: 1.000059]\n",
      "904 [D loss: 0.999964] [G loss: 1.000061]\n",
      "905 [D loss: 0.999963] [G loss: 1.000068]\n",
      "906 [D loss: 0.999965] [G loss: 1.000070]\n",
      "907 [D loss: 0.999971] [G loss: 1.000060]\n",
      "908 [D loss: 0.999971] [G loss: 1.000064]\n",
      "909 [D loss: 0.999966] [G loss: 1.000072]\n",
      "910 [D loss: 0.999965] [G loss: 1.000065]\n",
      "911 [D loss: 0.999979] [G loss: 1.000079]\n",
      "912 [D loss: 0.999966] [G loss: 1.000067]\n",
      "913 [D loss: 0.999971] [G loss: 1.000059]\n",
      "914 [D loss: 0.999973] [G loss: 1.000062]\n",
      "915 [D loss: 0.999964] [G loss: 1.000070]\n",
      "916 [D loss: 0.999964] [G loss: 1.000074]\n",
      "917 [D loss: 0.999970] [G loss: 1.000067]\n",
      "918 [D loss: 0.999967] [G loss: 1.000057]\n",
      "919 [D loss: 0.999964] [G loss: 1.000056]\n",
      "920 [D loss: 0.999961] [G loss: 1.000066]\n",
      "921 [D loss: 0.999968] [G loss: 1.000068]\n",
      "922 [D loss: 0.999962] [G loss: 1.000066]\n",
      "923 [D loss: 0.999965] [G loss: 1.000063]\n",
      "924 [D loss: 0.999965] [G loss: 1.000060]\n",
      "925 [D loss: 0.999967] [G loss: 1.000063]\n",
      "926 [D loss: 0.999967] [G loss: 1.000073]\n",
      "927 [D loss: 0.999976] [G loss: 1.000063]\n",
      "928 [D loss: 0.999973] [G loss: 1.000069]\n",
      "929 [D loss: 0.999966] [G loss: 1.000071]\n",
      "930 [D loss: 0.999966] [G loss: 1.000057]\n",
      "931 [D loss: 0.999977] [G loss: 1.000058]\n",
      "932 [D loss: 0.999965] [G loss: 1.000066]\n",
      "933 [D loss: 0.999976] [G loss: 1.000063]\n",
      "934 [D loss: 0.999973] [G loss: 1.000054]\n",
      "935 [D loss: 0.999975] [G loss: 1.000069]\n",
      "936 [D loss: 0.999975] [G loss: 1.000051]\n",
      "937 [D loss: 0.999966] [G loss: 1.000057]\n",
      "938 [D loss: 0.999971] [G loss: 1.000066]\n",
      "939 [D loss: 0.999972] [G loss: 1.000076]\n",
      "940 [D loss: 0.999972] [G loss: 1.000066]\n",
      "941 [D loss: 0.999976] [G loss: 1.000061]\n",
      "942 [D loss: 0.999969] [G loss: 1.000061]\n",
      "943 [D loss: 0.999967] [G loss: 1.000058]\n",
      "944 [D loss: 0.999968] [G loss: 1.000066]\n",
      "945 [D loss: 0.999970] [G loss: 1.000073]\n",
      "946 [D loss: 0.999974] [G loss: 1.000066]\n",
      "947 [D loss: 0.999980] [G loss: 1.000066]\n",
      "948 [D loss: 0.999968] [G loss: 1.000058]\n",
      "949 [D loss: 0.999965] [G loss: 1.000064]\n",
      "950 [D loss: 0.999962] [G loss: 1.000064]\n",
      "951 [D loss: 0.999968] [G loss: 1.000073]\n",
      "952 [D loss: 0.999969] [G loss: 1.000076]\n",
      "953 [D loss: 0.999979] [G loss: 1.000061]\n",
      "954 [D loss: 0.999964] [G loss: 1.000057]\n",
      "955 [D loss: 0.999969] [G loss: 1.000067]\n",
      "956 [D loss: 0.999971] [G loss: 1.000073]\n",
      "957 [D loss: 0.999975] [G loss: 1.000067]\n",
      "958 [D loss: 0.999971] [G loss: 1.000070]\n",
      "959 [D loss: 0.999972] [G loss: 1.000071]\n",
      "960 [D loss: 0.999970] [G loss: 1.000059]\n",
      "961 [D loss: 0.999975] [G loss: 1.000067]\n",
      "962 [D loss: 0.999978] [G loss: 1.000061]\n",
      "963 [D loss: 0.999973] [G loss: 1.000067]\n",
      "964 [D loss: 0.999973] [G loss: 1.000065]\n",
      "965 [D loss: 0.999975] [G loss: 1.000061]\n",
      "966 [D loss: 0.999970] [G loss: 1.000055]\n",
      "967 [D loss: 0.999966] [G loss: 1.000057]\n",
      "968 [D loss: 0.999976] [G loss: 1.000047]\n",
      "969 [D loss: 0.999973] [G loss: 1.000062]\n",
      "970 [D loss: 0.999974] [G loss: 1.000045]\n",
      "971 [D loss: 0.999962] [G loss: 1.000061]\n",
      "972 [D loss: 0.999958] [G loss: 1.000066]\n",
      "973 [D loss: 0.999960] [G loss: 1.000055]\n",
      "974 [D loss: 0.999968] [G loss: 1.000078]\n",
      "975 [D loss: 0.999974] [G loss: 1.000070]\n",
      "976 [D loss: 0.999970] [G loss: 1.000051]\n",
      "977 [D loss: 0.999973] [G loss: 1.000062]\n",
      "978 [D loss: 0.999968] [G loss: 1.000061]\n",
      "979 [D loss: 0.999973] [G loss: 1.000066]\n",
      "980 [D loss: 0.999975] [G loss: 1.000058]\n",
      "981 [D loss: 0.999980] [G loss: 1.000061]\n",
      "982 [D loss: 0.999976] [G loss: 1.000051]\n",
      "983 [D loss: 0.999969] [G loss: 1.000060]\n",
      "984 [D loss: 0.999962] [G loss: 1.000065]\n",
      "985 [D loss: 0.999964] [G loss: 1.000060]\n",
      "986 [D loss: 0.999963] [G loss: 1.000065]\n",
      "987 [D loss: 0.999965] [G loss: 1.000055]\n",
      "988 [D loss: 0.999970] [G loss: 1.000064]\n",
      "989 [D loss: 0.999968] [G loss: 1.000061]\n",
      "990 [D loss: 0.999971] [G loss: 1.000068]\n",
      "991 [D loss: 0.999969] [G loss: 1.000064]\n",
      "992 [D loss: 0.999968] [G loss: 1.000058]\n",
      "993 [D loss: 0.999966] [G loss: 1.000072]\n",
      "994 [D loss: 0.999973] [G loss: 1.000065]\n",
      "995 [D loss: 0.999977] [G loss: 1.000065]\n",
      "996 [D loss: 0.999964] [G loss: 1.000059]\n",
      "997 [D loss: 0.999967] [G loss: 1.000072]\n",
      "998 [D loss: 0.999976] [G loss: 1.000049]\n",
      "999 [D loss: 0.999963] [G loss: 1.000055]\n",
      "1000 [D loss: 0.999972] [G loss: 1.000070]\n",
      "1001 [D loss: 0.999965] [G loss: 1.000053]\n",
      "1002 [D loss: 0.999970] [G loss: 1.000073]\n",
      "1003 [D loss: 0.999964] [G loss: 1.000054]\n",
      "1004 [D loss: 0.999969] [G loss: 1.000052]\n",
      "1005 [D loss: 0.999973] [G loss: 1.000068]\n",
      "1006 [D loss: 0.999968] [G loss: 1.000072]\n",
      "1007 [D loss: 0.999965] [G loss: 1.000064]\n",
      "1008 [D loss: 0.999976] [G loss: 1.000065]\n",
      "1009 [D loss: 0.999967] [G loss: 1.000058]\n",
      "1010 [D loss: 0.999975] [G loss: 1.000060]\n",
      "1011 [D loss: 0.999973] [G loss: 1.000060]\n",
      "1012 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1013 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1014 [D loss: 0.999968] [G loss: 1.000057]\n",
      "1015 [D loss: 0.999969] [G loss: 1.000061]\n",
      "1016 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1017 [D loss: 0.999966] [G loss: 1.000065]\n",
      "1018 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1019 [D loss: 0.999969] [G loss: 1.000081]\n",
      "1020 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1021 [D loss: 0.999966] [G loss: 1.000062]\n",
      "1022 [D loss: 0.999967] [G loss: 1.000062]\n",
      "1023 [D loss: 0.999976] [G loss: 1.000067]\n",
      "1024 [D loss: 0.999966] [G loss: 1.000060]\n",
      "1025 [D loss: 0.999974] [G loss: 1.000054]\n",
      "1026 [D loss: 0.999966] [G loss: 1.000069]\n",
      "1027 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1028 [D loss: 0.999971] [G loss: 1.000060]\n",
      "1029 [D loss: 0.999967] [G loss: 1.000061]\n",
      "1030 [D loss: 0.999979] [G loss: 1.000062]\n",
      "1031 [D loss: 0.999962] [G loss: 1.000074]\n",
      "1032 [D loss: 0.999973] [G loss: 1.000068]\n",
      "1033 [D loss: 0.999967] [G loss: 1.000073]\n",
      "1034 [D loss: 0.999978] [G loss: 1.000062]\n",
      "1035 [D loss: 0.999965] [G loss: 1.000062]\n",
      "1036 [D loss: 0.999975] [G loss: 1.000064]\n",
      "1037 [D loss: 0.999978] [G loss: 1.000063]\n",
      "1038 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1039 [D loss: 0.999973] [G loss: 1.000074]\n",
      "1040 [D loss: 0.999972] [G loss: 1.000054]\n",
      "1041 [D loss: 0.999971] [G loss: 1.000052]\n",
      "1042 [D loss: 0.999971] [G loss: 1.000071]\n",
      "1043 [D loss: 0.999965] [G loss: 1.000058]\n",
      "1044 [D loss: 0.999969] [G loss: 1.000071]\n",
      "1045 [D loss: 0.999975] [G loss: 1.000052]\n",
      "1046 [D loss: 0.999971] [G loss: 1.000067]\n",
      "1047 [D loss: 0.999965] [G loss: 1.000061]\n",
      "1048 [D loss: 0.999968] [G loss: 1.000062]\n",
      "1049 [D loss: 0.999970] [G loss: 1.000056]\n",
      "1050 [D loss: 0.999976] [G loss: 1.000064]\n",
      "1051 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1052 [D loss: 0.999971] [G loss: 1.000058]\n",
      "1053 [D loss: 0.999967] [G loss: 1.000057]\n",
      "1054 [D loss: 0.999976] [G loss: 1.000065]\n",
      "1055 [D loss: 0.999967] [G loss: 1.000073]\n",
      "1056 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1057 [D loss: 0.999970] [G loss: 1.000052]\n",
      "1058 [D loss: 0.999967] [G loss: 1.000068]\n",
      "1059 [D loss: 0.999962] [G loss: 1.000064]\n",
      "1060 [D loss: 0.999968] [G loss: 1.000058]\n",
      "1061 [D loss: 0.999965] [G loss: 1.000069]\n",
      "1062 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1063 [D loss: 0.999967] [G loss: 1.000056]\n",
      "1064 [D loss: 0.999975] [G loss: 1.000067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1065 [D loss: 0.999969] [G loss: 1.000058]\n",
      "1066 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1067 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1068 [D loss: 0.999966] [G loss: 1.000064]\n",
      "1069 [D loss: 0.999970] [G loss: 1.000055]\n",
      "1070 [D loss: 0.999969] [G loss: 1.000059]\n",
      "1071 [D loss: 0.999961] [G loss: 1.000064]\n",
      "1072 [D loss: 0.999976] [G loss: 1.000057]\n",
      "1073 [D loss: 0.999976] [G loss: 1.000066]\n",
      "1074 [D loss: 0.999965] [G loss: 1.000077]\n",
      "1075 [D loss: 0.999968] [G loss: 1.000056]\n",
      "1076 [D loss: 0.999963] [G loss: 1.000064]\n",
      "1077 [D loss: 0.999980] [G loss: 1.000061]\n",
      "1078 [D loss: 0.999974] [G loss: 1.000080]\n",
      "1079 [D loss: 0.999964] [G loss: 1.000054]\n",
      "1080 [D loss: 0.999974] [G loss: 1.000056]\n",
      "1081 [D loss: 0.999981] [G loss: 1.000067]\n",
      "1082 [D loss: 0.999963] [G loss: 1.000064]\n",
      "1083 [D loss: 0.999974] [G loss: 1.000059]\n",
      "1084 [D loss: 0.999970] [G loss: 1.000051]\n",
      "1085 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1086 [D loss: 0.999974] [G loss: 1.000060]\n",
      "1087 [D loss: 0.999965] [G loss: 1.000046]\n",
      "1088 [D loss: 0.999968] [G loss: 1.000060]\n",
      "1089 [D loss: 0.999974] [G loss: 1.000061]\n",
      "1090 [D loss: 0.999964] [G loss: 1.000064]\n",
      "1091 [D loss: 0.999971] [G loss: 1.000070]\n",
      "1092 [D loss: 0.999979] [G loss: 1.000058]\n",
      "1093 [D loss: 0.999967] [G loss: 1.000065]\n",
      "1094 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1095 [D loss: 0.999967] [G loss: 1.000062]\n",
      "1096 [D loss: 0.999968] [G loss: 1.000063]\n",
      "1097 [D loss: 0.999963] [G loss: 1.000059]\n",
      "1098 [D loss: 0.999964] [G loss: 1.000065]\n",
      "1099 [D loss: 0.999968] [G loss: 1.000061]\n",
      "1100 [D loss: 0.999969] [G loss: 1.000060]\n",
      "1101 [D loss: 0.999964] [G loss: 1.000059]\n",
      "1102 [D loss: 0.999978] [G loss: 1.000071]\n",
      "1103 [D loss: 0.999968] [G loss: 1.000059]\n",
      "1104 [D loss: 0.999975] [G loss: 1.000060]\n",
      "1105 [D loss: 0.999971] [G loss: 1.000053]\n",
      "1106 [D loss: 0.999972] [G loss: 1.000052]\n",
      "1107 [D loss: 0.999976] [G loss: 1.000071]\n",
      "1108 [D loss: 0.999962] [G loss: 1.000067]\n",
      "1109 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1110 [D loss: 0.999975] [G loss: 1.000059]\n",
      "1111 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1112 [D loss: 0.999982] [G loss: 1.000057]\n",
      "1113 [D loss: 0.999972] [G loss: 1.000058]\n",
      "1114 [D loss: 0.999970] [G loss: 1.000052]\n",
      "1115 [D loss: 0.999974] [G loss: 1.000078]\n",
      "1116 [D loss: 0.999976] [G loss: 1.000070]\n",
      "1117 [D loss: 0.999974] [G loss: 1.000058]\n",
      "1118 [D loss: 0.999977] [G loss: 1.000048]\n",
      "1119 [D loss: 0.999975] [G loss: 1.000062]\n",
      "1120 [D loss: 0.999962] [G loss: 1.000062]\n",
      "1121 [D loss: 0.999975] [G loss: 1.000061]\n",
      "1122 [D loss: 0.999974] [G loss: 1.000063]\n",
      "1123 [D loss: 0.999965] [G loss: 1.000073]\n",
      "1124 [D loss: 0.999976] [G loss: 1.000062]\n",
      "1125 [D loss: 0.999969] [G loss: 1.000070]\n",
      "1126 [D loss: 0.999973] [G loss: 1.000066]\n",
      "1127 [D loss: 0.999969] [G loss: 1.000064]\n",
      "1128 [D loss: 0.999966] [G loss: 1.000056]\n",
      "1129 [D loss: 0.999967] [G loss: 1.000057]\n",
      "1130 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1131 [D loss: 0.999974] [G loss: 1.000063]\n",
      "1132 [D loss: 0.999971] [G loss: 1.000050]\n",
      "1133 [D loss: 0.999964] [G loss: 1.000063]\n",
      "1134 [D loss: 0.999967] [G loss: 1.000065]\n",
      "1135 [D loss: 0.999976] [G loss: 1.000060]\n",
      "1136 [D loss: 0.999962] [G loss: 1.000063]\n",
      "1137 [D loss: 0.999973] [G loss: 1.000057]\n",
      "1138 [D loss: 0.999970] [G loss: 1.000058]\n",
      "1139 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1140 [D loss: 0.999966] [G loss: 1.000047]\n",
      "1141 [D loss: 0.999972] [G loss: 1.000050]\n",
      "1142 [D loss: 0.999960] [G loss: 1.000057]\n",
      "1143 [D loss: 0.999970] [G loss: 1.000049]\n",
      "1144 [D loss: 0.999975] [G loss: 1.000065]\n",
      "1145 [D loss: 0.999966] [G loss: 1.000055]\n",
      "1146 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1147 [D loss: 0.999972] [G loss: 1.000053]\n",
      "1148 [D loss: 0.999969] [G loss: 1.000063]\n",
      "1149 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1150 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1151 [D loss: 0.999972] [G loss: 1.000072]\n",
      "1152 [D loss: 0.999979] [G loss: 1.000060]\n",
      "1153 [D loss: 0.999968] [G loss: 1.000059]\n",
      "1154 [D loss: 0.999971] [G loss: 1.000056]\n",
      "1155 [D loss: 0.999967] [G loss: 1.000056]\n",
      "1156 [D loss: 0.999972] [G loss: 1.000053]\n",
      "1157 [D loss: 0.999969] [G loss: 1.000060]\n",
      "1158 [D loss: 0.999971] [G loss: 1.000069]\n",
      "1159 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1160 [D loss: 0.999984] [G loss: 1.000066]\n",
      "1161 [D loss: 0.999979] [G loss: 1.000064]\n",
      "1162 [D loss: 0.999968] [G loss: 1.000053]\n",
      "1163 [D loss: 0.999974] [G loss: 1.000060]\n",
      "1164 [D loss: 0.999975] [G loss: 1.000062]\n",
      "1165 [D loss: 0.999975] [G loss: 1.000053]\n",
      "1166 [D loss: 0.999970] [G loss: 1.000075]\n",
      "1167 [D loss: 0.999975] [G loss: 1.000078]\n",
      "1168 [D loss: 0.999960] [G loss: 1.000062]\n",
      "1169 [D loss: 0.999962] [G loss: 1.000067]\n",
      "1170 [D loss: 0.999970] [G loss: 1.000070]\n",
      "1171 [D loss: 0.999961] [G loss: 1.000053]\n",
      "1172 [D loss: 0.999963] [G loss: 1.000072]\n",
      "1173 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1174 [D loss: 0.999974] [G loss: 1.000063]\n",
      "1175 [D loss: 0.999969] [G loss: 1.000061]\n",
      "1176 [D loss: 0.999979] [G loss: 1.000068]\n",
      "1177 [D loss: 0.999970] [G loss: 1.000066]\n",
      "1178 [D loss: 0.999973] [G loss: 1.000068]\n",
      "1179 [D loss: 0.999969] [G loss: 1.000069]\n",
      "1180 [D loss: 0.999978] [G loss: 1.000060]\n",
      "1181 [D loss: 0.999963] [G loss: 1.000062]\n",
      "1182 [D loss: 0.999961] [G loss: 1.000060]\n",
      "1183 [D loss: 0.999967] [G loss: 1.000063]\n",
      "1184 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1185 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1186 [D loss: 0.999974] [G loss: 1.000063]\n",
      "1187 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1188 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1189 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1190 [D loss: 0.999965] [G loss: 1.000075]\n",
      "1191 [D loss: 0.999968] [G loss: 1.000064]\n",
      "1192 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1193 [D loss: 0.999975] [G loss: 1.000066]\n",
      "1194 [D loss: 0.999972] [G loss: 1.000062]\n",
      "1195 [D loss: 0.999975] [G loss: 1.000062]\n",
      "1196 [D loss: 0.999972] [G loss: 1.000066]\n",
      "1197 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1198 [D loss: 0.999971] [G loss: 1.000068]\n",
      "1199 [D loss: 0.999973] [G loss: 1.000075]\n",
      "1200 [D loss: 0.999960] [G loss: 1.000065]\n",
      "1201 [D loss: 0.999966] [G loss: 1.000057]\n",
      "1202 [D loss: 0.999963] [G loss: 1.000069]\n",
      "1203 [D loss: 0.999975] [G loss: 1.000061]\n",
      "1204 [D loss: 0.999971] [G loss: 1.000077]\n",
      "1205 [D loss: 0.999969] [G loss: 1.000057]\n",
      "1206 [D loss: 0.999964] [G loss: 1.000077]\n",
      "1207 [D loss: 0.999970] [G loss: 1.000058]\n",
      "1208 [D loss: 0.999975] [G loss: 1.000066]\n",
      "1209 [D loss: 0.999975] [G loss: 1.000067]\n",
      "1210 [D loss: 0.999961] [G loss: 1.000060]\n",
      "1211 [D loss: 0.999968] [G loss: 1.000053]\n",
      "1212 [D loss: 0.999968] [G loss: 1.000056]\n",
      "1213 [D loss: 0.999970] [G loss: 1.000064]\n",
      "1214 [D loss: 0.999960] [G loss: 1.000069]\n",
      "1215 [D loss: 0.999975] [G loss: 1.000065]\n",
      "1216 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1217 [D loss: 0.999974] [G loss: 1.000055]\n",
      "1218 [D loss: 0.999967] [G loss: 1.000053]\n",
      "1219 [D loss: 0.999967] [G loss: 1.000061]\n",
      "1220 [D loss: 0.999965] [G loss: 1.000057]\n",
      "1221 [D loss: 0.999974] [G loss: 1.000075]\n",
      "1222 [D loss: 0.999973] [G loss: 1.000059]\n",
      "1223 [D loss: 0.999975] [G loss: 1.000064]\n",
      "1224 [D loss: 0.999972] [G loss: 1.000048]\n",
      "1225 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1226 [D loss: 0.999973] [G loss: 1.000061]\n",
      "1227 [D loss: 0.999972] [G loss: 1.000067]\n",
      "1228 [D loss: 0.999970] [G loss: 1.000061]\n",
      "1229 [D loss: 0.999965] [G loss: 1.000065]\n",
      "1230 [D loss: 0.999978] [G loss: 1.000062]\n",
      "1231 [D loss: 0.999970] [G loss: 1.000067]\n",
      "1232 [D loss: 0.999975] [G loss: 1.000062]\n",
      "1233 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1234 [D loss: 0.999968] [G loss: 1.000065]\n",
      "1235 [D loss: 0.999973] [G loss: 1.000056]\n",
      "1236 [D loss: 0.999968] [G loss: 1.000048]\n",
      "1237 [D loss: 0.999968] [G loss: 1.000058]\n",
      "1238 [D loss: 0.999970] [G loss: 1.000057]\n",
      "1239 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1240 [D loss: 0.999977] [G loss: 1.000062]\n",
      "1241 [D loss: 0.999968] [G loss: 1.000056]\n",
      "1242 [D loss: 0.999971] [G loss: 1.000059]\n",
      "1243 [D loss: 0.999975] [G loss: 1.000059]\n",
      "1244 [D loss: 0.999970] [G loss: 1.000051]\n",
      "1245 [D loss: 0.999973] [G loss: 1.000057]\n",
      "1246 [D loss: 0.999970] [G loss: 1.000056]\n",
      "1247 [D loss: 0.999964] [G loss: 1.000069]\n",
      "1248 [D loss: 0.999972] [G loss: 1.000053]\n",
      "1249 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1250 [D loss: 0.999969] [G loss: 1.000061]\n",
      "1251 [D loss: 0.999965] [G loss: 1.000049]\n",
      "1252 [D loss: 0.999968] [G loss: 1.000058]\n",
      "1253 [D loss: 0.999971] [G loss: 1.000070]\n",
      "1254 [D loss: 0.999969] [G loss: 1.000055]\n",
      "1255 [D loss: 0.999970] [G loss: 1.000057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1256 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1257 [D loss: 0.999963] [G loss: 1.000070]\n",
      "1258 [D loss: 0.999977] [G loss: 1.000068]\n",
      "1259 [D loss: 0.999968] [G loss: 1.000077]\n",
      "1260 [D loss: 0.999963] [G loss: 1.000068]\n",
      "1261 [D loss: 0.999967] [G loss: 1.000059]\n",
      "1262 [D loss: 0.999970] [G loss: 1.000063]\n",
      "1263 [D loss: 0.999969] [G loss: 1.000059]\n",
      "1264 [D loss: 0.999971] [G loss: 1.000058]\n",
      "1265 [D loss: 0.999971] [G loss: 1.000061]\n",
      "1266 [D loss: 0.999971] [G loss: 1.000074]\n",
      "1267 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1268 [D loss: 0.999971] [G loss: 1.000066]\n",
      "1269 [D loss: 0.999972] [G loss: 1.000057]\n",
      "1270 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1271 [D loss: 0.999971] [G loss: 1.000074]\n",
      "1272 [D loss: 0.999977] [G loss: 1.000065]\n",
      "1273 [D loss: 0.999969] [G loss: 1.000062]\n",
      "1274 [D loss: 0.999975] [G loss: 1.000057]\n",
      "1275 [D loss: 0.999970] [G loss: 1.000074]\n",
      "1276 [D loss: 0.999962] [G loss: 1.000070]\n",
      "1277 [D loss: 0.999966] [G loss: 1.000061]\n",
      "1278 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1279 [D loss: 0.999970] [G loss: 1.000062]\n",
      "1280 [D loss: 0.999974] [G loss: 1.000064]\n",
      "1281 [D loss: 0.999971] [G loss: 1.000053]\n",
      "1282 [D loss: 0.999969] [G loss: 1.000058]\n",
      "1283 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1284 [D loss: 0.999970] [G loss: 1.000058]\n",
      "1285 [D loss: 0.999974] [G loss: 1.000056]\n",
      "1286 [D loss: 0.999969] [G loss: 1.000067]\n",
      "1287 [D loss: 0.999970] [G loss: 1.000065]\n",
      "1288 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1289 [D loss: 0.999973] [G loss: 1.000058]\n",
      "1290 [D loss: 0.999967] [G loss: 1.000055]\n",
      "1291 [D loss: 0.999973] [G loss: 1.000060]\n",
      "1292 [D loss: 0.999967] [G loss: 1.000069]\n",
      "1293 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1294 [D loss: 0.999970] [G loss: 1.000051]\n",
      "1295 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1296 [D loss: 0.999978] [G loss: 1.000066]\n",
      "1297 [D loss: 0.999966] [G loss: 1.000064]\n",
      "1298 [D loss: 0.999966] [G loss: 1.000061]\n",
      "1299 [D loss: 0.999976] [G loss: 1.000062]\n",
      "1300 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1301 [D loss: 0.999973] [G loss: 1.000056]\n",
      "1302 [D loss: 0.999970] [G loss: 1.000071]\n",
      "1303 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1304 [D loss: 0.999965] [G loss: 1.000063]\n",
      "1305 [D loss: 0.999965] [G loss: 1.000066]\n",
      "1306 [D loss: 0.999977] [G loss: 1.000057]\n",
      "1307 [D loss: 0.999971] [G loss: 1.000064]\n",
      "1308 [D loss: 0.999975] [G loss: 1.000075]\n",
      "1309 [D loss: 0.999970] [G loss: 1.000060]\n",
      "1310 [D loss: 0.999977] [G loss: 1.000069]\n",
      "1311 [D loss: 0.999968] [G loss: 1.000050]\n",
      "1312 [D loss: 0.999978] [G loss: 1.000055]\n",
      "1313 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1314 [D loss: 0.999961] [G loss: 1.000073]\n",
      "1315 [D loss: 0.999979] [G loss: 1.000062]\n",
      "1316 [D loss: 0.999972] [G loss: 1.000047]\n",
      "1317 [D loss: 0.999968] [G loss: 1.000050]\n",
      "1318 [D loss: 0.999969] [G loss: 1.000061]\n",
      "1319 [D loss: 0.999965] [G loss: 1.000062]\n",
      "1320 [D loss: 0.999958] [G loss: 1.000077]\n",
      "1321 [D loss: 0.999963] [G loss: 1.000061]\n",
      "1322 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1323 [D loss: 0.999961] [G loss: 1.000068]\n",
      "1324 [D loss: 0.999969] [G loss: 1.000066]\n",
      "1325 [D loss: 0.999966] [G loss: 1.000067]\n",
      "1326 [D loss: 0.999969] [G loss: 1.000072]\n",
      "1327 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1328 [D loss: 0.999971] [G loss: 1.000074]\n",
      "1329 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1330 [D loss: 0.999973] [G loss: 1.000059]\n",
      "1331 [D loss: 0.999968] [G loss: 1.000066]\n",
      "1332 [D loss: 0.999968] [G loss: 1.000061]\n",
      "1333 [D loss: 0.999965] [G loss: 1.000065]\n",
      "1334 [D loss: 0.999967] [G loss: 1.000070]\n",
      "1335 [D loss: 0.999972] [G loss: 1.000058]\n",
      "1336 [D loss: 0.999981] [G loss: 1.000062]\n",
      "1337 [D loss: 0.999972] [G loss: 1.000065]\n",
      "1338 [D loss: 0.999973] [G loss: 1.000075]\n",
      "1339 [D loss: 0.999973] [G loss: 1.000064]\n",
      "1340 [D loss: 0.999963] [G loss: 1.000053]\n",
      "1341 [D loss: 0.999969] [G loss: 1.000054]\n",
      "1342 [D loss: 0.999966] [G loss: 1.000064]\n",
      "1343 [D loss: 0.999964] [G loss: 1.000065]\n",
      "1344 [D loss: 0.999973] [G loss: 1.000065]\n",
      "1345 [D loss: 0.999973] [G loss: 1.000053]\n",
      "1346 [D loss: 0.999974] [G loss: 1.000059]\n",
      "1347 [D loss: 0.999966] [G loss: 1.000057]\n",
      "1348 [D loss: 0.999968] [G loss: 1.000062]\n",
      "1349 [D loss: 0.999964] [G loss: 1.000055]\n",
      "1350 [D loss: 0.999966] [G loss: 1.000055]\n",
      "1351 [D loss: 0.999970] [G loss: 1.000057]\n",
      "1352 [D loss: 0.999969] [G loss: 1.000057]\n",
      "1353 [D loss: 0.999962] [G loss: 1.000065]\n",
      "1354 [D loss: 0.999965] [G loss: 1.000063]\n",
      "1355 [D loss: 0.999970] [G loss: 1.000068]\n",
      "1356 [D loss: 0.999972] [G loss: 1.000061]\n",
      "1357 [D loss: 0.999980] [G loss: 1.000060]\n",
      "1358 [D loss: 0.999973] [G loss: 1.000063]\n",
      "1359 [D loss: 0.999974] [G loss: 1.000061]\n",
      "1360 [D loss: 0.999974] [G loss: 1.000042]\n",
      "1361 [D loss: 0.999964] [G loss: 1.000064]\n",
      "1362 [D loss: 0.999967] [G loss: 1.000056]\n",
      "1363 [D loss: 0.999971] [G loss: 1.000054]\n",
      "1364 [D loss: 0.999974] [G loss: 1.000055]\n",
      "1365 [D loss: 0.999971] [G loss: 1.000062]\n",
      "1366 [D loss: 0.999971] [G loss: 1.000065]\n",
      "1367 [D loss: 0.999971] [G loss: 1.000063]\n",
      "1368 [D loss: 0.999967] [G loss: 1.000060]\n",
      "1369 [D loss: 0.999980] [G loss: 1.000058]\n",
      "1370 [D loss: 0.999964] [G loss: 1.000062]\n",
      "1371 [D loss: 0.999964] [G loss: 1.000067]\n",
      "1372 [D loss: 0.999978] [G loss: 1.000061]\n",
      "1373 [D loss: 0.999960] [G loss: 1.000050]\n",
      "1374 [D loss: 0.999972] [G loss: 1.000044]\n",
      "1375 [D loss: 0.999965] [G loss: 1.000053]\n",
      "1376 [D loss: 0.999971] [G loss: 1.000055]\n",
      "1377 [D loss: 0.999975] [G loss: 1.000074]\n",
      "1378 [D loss: 0.999973] [G loss: 1.000062]\n",
      "1379 [D loss: 0.999966] [G loss: 1.000062]\n",
      "1380 [D loss: 0.999963] [G loss: 1.000071]\n",
      "1381 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1382 [D loss: 0.999977] [G loss: 1.000068]\n",
      "1383 [D loss: 0.999973] [G loss: 1.000058]\n",
      "1384 [D loss: 0.999966] [G loss: 1.000075]\n",
      "1385 [D loss: 0.999962] [G loss: 1.000073]\n",
      "1386 [D loss: 0.999975] [G loss: 1.000063]\n",
      "1387 [D loss: 0.999966] [G loss: 1.000065]\n",
      "1388 [D loss: 0.999972] [G loss: 1.000063]\n",
      "1389 [D loss: 0.999965] [G loss: 1.000058]\n",
      "1390 [D loss: 0.999972] [G loss: 1.000051]\n",
      "1391 [D loss: 0.999965] [G loss: 1.000060]\n",
      "1392 [D loss: 0.999973] [G loss: 1.000067]\n",
      "1393 [D loss: 0.999977] [G loss: 1.000051]\n",
      "1394 [D loss: 0.999968] [G loss: 1.000044]\n",
      "1395 [D loss: 0.999964] [G loss: 1.000056]\n",
      "1396 [D loss: 0.999978] [G loss: 1.000057]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-2cf451ccc359>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[0mwgan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWGAN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 176\u001b[1;33m \u001b[0mwgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-2cf451ccc359>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                 \u001b[1;31m# Generate a batch of new images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[0mgen_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m                 \u001b[1;31m# Train the critic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1169\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic()\n",
    "        self.critic.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(img)\n",
    "\n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss=self.wasserstein_loss,\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=4, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs = X_train[idx]\n",
    "                \n",
    "                # Sample noise as generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"data/demo6/WGAN/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "wgan = WGAN()\n",
    "wgan.train(epochs=4000, batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
